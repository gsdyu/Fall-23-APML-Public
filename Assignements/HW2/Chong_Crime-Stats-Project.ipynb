{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55e6b918-afac-43e8-b338-43f8ac64014f",
   "metadata": {},
   "source": [
    "##  Deep Neural Networks Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8e066d-2849-4a6e-9c71-bb98d605ae07",
   "metadata": {},
   "source": [
    "In this project, you will be working with a real-world data set from the Las Vegas Metropolitan Police Department. The dataset  contains information about the reported incidents, including the time and location of the crime, type of incident, and number of persons involved. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e87fac7-352a-4c39-b087-76254b5e2743",
   "metadata": {},
   "source": [
    "The dataset is downloaded from the public docket at: \n",
    "https://opendata-lvmpd.hub.arcgis.com\n",
    "\n",
    "let's read the csv file and transform the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "637211a4-582f-426b-a127-c3f284463f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import sys\n",
    "from torch.nn import BatchNorm1d, Module, ReLU, Linear, MSELoss, CrossEntropyLoss, Sequential, NLLLoss\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bcf40b02-80b6-4abc-a662-f7ed50a65181",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_df = pd.read_csv('LVMPD-Stats.csv', parse_dates=['ReportedOn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e1ca1d15-3955-4971-a3c4-c1a73b62edda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         Friday\n",
      "1       Thursday\n",
      "2        Tuesday\n",
      "3       Thursday\n",
      "4       Thursday\n",
      "         ...    \n",
      "270       Monday\n",
      "271       Monday\n",
      "272     Thursday\n",
      "273    Wednesday\n",
      "274    Wednesday\n",
      "Name: DayOfWeek, Length: 275, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('LVMPD-Stats.csv', parse_dates=['ReportedOn'],\n",
    "                 usecols = ['X', 'Y', 'ReportedOn',\n",
    "                            'Area_Command','NIBRSOffenseCode',\n",
    "                            'VictimCount' ] )\n",
    "\n",
    "df['DayOfWeek'] = df['ReportedOn'].dt.day_name()\n",
    "print(df['DayOfWeek'])\n",
    "df['Time' ]     = df['ReportedOn'].dt.hour\n",
    "df.drop(columns = 'ReportedOn', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3ddc413d-ba3f-4204-bc18-7fdd4de8d221",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try to run this cell only once or else var unique_days will be messed up\n",
    "df['X'] = df['X'] \n",
    "df['Y'] = df['Y'] \n",
    "df['Time'] = pd.factorize(df['Time'])[0]\n",
    "#took the index of unique for the days;used for the analysis of task 6\n",
    "#if unique_days not giving string of the days but instead indexes, need to reread the data (run previous cells again)\n",
    "df['DayOfWeek'], unique_days = pd.factorize(df['DayOfWeek'])\n",
    "df.Area_Command = pd.factorize(df['Area_Command'])[0]\n",
    "df.VictimCount, unique_people = pd.factorize(df['VictimCount']) \n",
    "df.NIBRSOffenseCode, area = pd.factorize(df['NIBRSOffenseCode'])\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a9c6162f-9686-4195-818d-950a6368c686",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df[['X', 'Y', 'Area_Command', 'NIBRSOffenseCode',\n",
    "       'DayOfWeek', 'Time','VictimCount']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a90bc78a-6d1b-4fe4-a1b0-8333aec1c851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(275, 7)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651605b1-8d2c-4d3e-a09e-9aef6e550fc6",
   "metadata": {},
   "source": [
    "# Goal\n",
    "The goal is to build a predictive model that is trained on the following data:\n",
    "* latitude and longitude (location)\n",
    "* Hour of the day\n",
    "* Day of the week\n",
    "* Area-of-command code: The police designation of the bureau of the operation.\n",
    "* Classification code for the crime committed\n",
    "  \n",
    "The predicted variable is the number of persons involved in the accident.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e54f0b8-83f9-4db9-88f9-f5a595342069",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "* print a few rows of the values in the dataframe ``df`` and explain what each column of data means. \n",
    "* identify the input and target variables\n",
    "* what is the range of values in each column? Do you need to scale, shift or normalize your data? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a300c19e-0c15-4fc9-8bef-823345aa5374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      1\n",
       "2      2\n",
       "3      1\n",
       "4      1\n",
       "      ..\n",
       "270    5\n",
       "271    5\n",
       "272    1\n",
       "273    6\n",
       "274    6\n",
       "Name: DayOfWeek, Length: 275, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"DayOfWeek\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42800a15-c653-4c16-b0ae-dfb5f55351e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Head:\n",
      "            X          Y  Area_Command  NIBRSOffenseCode  DayOfWeek  Time  \\\n",
      "0 -115.087518  36.216702             0                 0          0     0   \n",
      "1 -115.240172  36.189693             1                 1          1     1   \n",
      "2 -115.143088  36.181329             2                 1          2     0   \n",
      "3 -115.225014  36.117633             3                 1          1     2   \n",
      "4 -115.176708  36.095967             4                 1          1     3   \n",
      "\n",
      "   VictimCount  \n",
      "0            0  \n",
      "1            0  \n",
      "2            1  \n",
      "3            2  \n",
      "4            0  \n",
      "\n",
      "Tail:\n",
      "              X          Y  Area_Command  NIBRSOffenseCode  DayOfWeek  Time  \\\n",
      "270 -115.114739  36.119592             5                 1          5    18   \n",
      "271 -115.080764  36.162648             0                 1          5    17   \n",
      "272 -115.172073  36.123012             4                 1          1    16   \n",
      "273 -115.152593  36.066073             5                 1          6    23   \n",
      "274 -115.060345  36.137296             9                 1          6    13   \n",
      "\n",
      "     VictimCount  \n",
      "270            0  \n",
      "271            0  \n",
      "272            2  \n",
      "273            0  \n",
      "274            1  \n"
     ]
    }
   ],
   "source": [
    "#Task 1\n",
    "#few row of the values:\n",
    "print(\"\\nHead:\")\n",
    "print(df.head())\n",
    "print(\"\\nTail:\")\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44e9202a-dce6-4e36-8f7f-b565fa17f134",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explain Columns of Data:\n",
    "#Looking at mainly the head and tail of the list:\n",
    "#The longitude and latitude (X and Y) seem to appear in similar areas with X roaming around -115 and Y roaming around 36. The main difference seem to\n",
    "#vary here in the decimals\n",
    "\n",
    "#Area_Command, NIBRSOFFENSECODE, DayOfWeek, Time, and VictimCount are factorized, meaning that the values are based of uniqueness \n",
    "#from the first few data sets rather  than being actual values. Hence the larger the values in the column appear means \n",
    "#that the actual values are more distinct/the dataset does not reside in common values.\n",
    "\n",
    "#Area_Command looks to appear to have multiple unique values compare to the rest (just based on head and tail rows) with the head starting off with \n",
    "#multiple unique values. Also in the tails for Area_Command, a 9 appears, showing that there is atleast 9 unique values. This seems interesting \n",
    "#as the physical location of X and Y do not appear to far off from one another in the rows, but already they have unique Area_Command; this may show\n",
    "#a hint about how this column may have a greater factor than the other columns consider (just an initial prediction).\n",
    "\n",
    "#NIBRSOFFENSECODE, unlike Area_Command appear to have very few unique values, with 0 just appearing one time and 1 appearing the rest. Unlike \n",
    "#Area_Command, I predict that this column will have a lower factor towards deciding the target than Area_Command\n",
    "\n",
    "#DayOfWeek appears to have a similar uniqueness rate as Area_Command but possibly little less as it is not confirmed that it goes to 9 like in \n",
    "#Area_Command. This however may just be the case for the head and tail of the df and not the whole picture.\n",
    "\n",
    "#Time appears to have the most uniqueness rate out of all the other columns, which goes up to atleast 23. This does make sense though as 23 is right \n",
    "#below 24 and there are 24 hours in general. This suggests that this dataset may cover all hours from 12 am to 12 pm. Sidenote 1 hour could be possibly\n",
    "#missing since only 23 is guarenteed from the tails\n",
    "\n",
    "#VictimCount appears to have little uniqueness rate being only slightly more than NIBRSOffenseCode. Looking back at the actual numbers for VictimCount \n",
    "#and knowing that there is small uniqueness, this suggests that people are generally not going on large murder sprees frequently. Killing of 1 or 2 \n",
    "#people seem to be common and possibly make sense sociologically as murderers generally target only 1 person because of personal issues and revenge or\n",
    "#some accident; people do not do random murder sprees where they just felt like killing an insane amount. Large murders would also have more unique\n",
    "#range, which is not seen here in this dataset based on heads and tails.\n",
    "\n",
    "#conclusion: should normalize the data by column as the ranges for all the columns\n",
    "#differ. Will use batch normalization to normalize by the mean rather than min-max norm as outliers has been shown to occur for a few of the features.\n",
    "\n",
    "#input: X, Y, Area_Command, NIBRSOffenseCode, DayOfWeek, Time, VictimCount\n",
    "#target: number of persons involved in the accident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e484e3e-af2e-4de6-acab-058007c981f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details:\n",
      "                X           Y  Area_Command  NIBRSOffenseCode   DayOfWeek  \\\n",
      "count  275.000000  275.000000    275.000000        275.000000  275.000000   \n",
      "mean  -115.159326   36.143360      3.978182          0.909091    2.981818   \n",
      "std      0.101294    0.118418      3.045799          0.334878    1.924590   \n",
      "min   -116.000000   35.068419      0.000000          0.000000    0.000000   \n",
      "25%   -115.209198   36.114704      1.000000          1.000000    1.000000   \n",
      "50%   -115.149945   36.152415      3.000000          1.000000    3.000000   \n",
      "75%   -115.105200   36.183854      6.000000          1.000000    5.000000   \n",
      "max   -114.625570   37.000000     11.000000          2.000000    6.000000   \n",
      "\n",
      "             Time  VictimCount  \n",
      "count  275.000000   275.000000  \n",
      "mean    11.236364     0.712727  \n",
      "std      7.039937     0.978427  \n",
      "min      0.000000     0.000000  \n",
      "25%      5.000000     0.000000  \n",
      "50%     11.000000     0.000000  \n",
      "75%     18.000000     1.000000  \n",
      "max     23.000000     6.000000  \n"
     ]
    }
   ],
   "source": [
    "#Range:\n",
    "print(\"Details:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5549ecc9-3c0b-4efa-9a1f-340a25a1e4be",
   "metadata": {},
   "source": [
    "## Task 2 \n",
    "\n",
    "* Create two `DataLoader` objects for training and testing based on the input and output variables. Pick a reasonable batch size and verify the shape of data by iterating over the one dataset and printing the shape of the batched data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00fe4287-934b-4799-9e43-c3571acfbab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.5701,  0.3567,  0.2906,  ...,  0.0121, -0.4857, -0.0214],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#275 is divisible to a whole number by 25, hence why 25 is chosen as batch_size\n",
    "batch_size = 25\n",
    "#training accounts for first 225 batches, testing accounts for last 50 batches\n",
    "training_input = torch.from_numpy(df.values[0:-50,:-1]).float()\n",
    "training_label = torch.from_numpy(df.values[0:-50,-1]).float()\n",
    "\n",
    "testing_input = torch.from_numpy(df.values[-50:,:-1]).float()\n",
    "testing_label = torch.from_numpy(df.values[-50:,-1]).float()\n",
    "ones = torch.ones(testing_input.shape[0],1)\n",
    "\n",
    "#reshape labels from 1d to 2d array for BatchNorm1d \n",
    "training_label = training_label.reshape(training_label.shape[0],1)\n",
    "testing_label = testing_label.reshape(testing_label.shape[0],1)\n",
    "np.set_printoptions(suppress=True)\n",
    "torch.set_printoptions(threshold=0)\n",
    "#batch normalize the data outside the dataset; alternative is to do inside the dataset\n",
    "m = BatchNorm1d(training_input.shape[1])\n",
    "n = BatchNorm1d(1)\n",
    "#chose to normalize both training and testing after the split rather than before (split is when dataset splits into training and testing datasets).\n",
    "#this is intended to avoid overfitting by having training not consider values shown into testing; case where the model gets an advantage/adjustment over\n",
    "#the testing dataset even though realistically future dataset is not known to be adjusted to.\n",
    "norm_training_input = m(training_input)\n",
    "norm_training_label = n(training_label)\n",
    "norm_testing_input = m(testing_input)\n",
    "norm_testing_label = n(testing_label)\n",
    "print(norm_training_input[:,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75e2b6ac-2077-4826-bd11-06d504c8b721",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the custom dataset\n",
    "class CrimeDataset(Dataset):\n",
    "    #does not do transform; should do transform next time\n",
    "    def __init__(self, input, label):\n",
    "        self.input = input\n",
    "        self.label = label\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "    def __getitem__(self, idx):\n",
    "        input = self.input[idx]\n",
    "        label = self.label[idx]\n",
    "        return input, label\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53e592a6-7bb3-4f1c-a368-7d1e68678201",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dataloaders for the training and testing models\n",
    "training_set = CrimeDataset(norm_training_input, norm_training_label)\n",
    "training_set = DataLoader(training_set, batch_size=batch_size)\n",
    "testing_set = CrimeDataset(norm_testing_input, norm_testing_label)\n",
    "testing_set = DataLoader(testing_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fd87fda-d5e7-4027-8a91-3393c7a81e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the shape of training_model:\n",
      "\n",
      "Input:\n",
      "torch.Size([25, 6])\n",
      "Label:\n",
      "torch.Size([25, 1])\n",
      "Input:\n",
      "torch.Size([25, 6])\n",
      "Label:\n",
      "torch.Size([25, 1])\n",
      "Input:\n",
      "torch.Size([25, 6])\n",
      "Label:\n",
      "torch.Size([25, 1])\n",
      "Input:\n",
      "torch.Size([25, 6])\n",
      "Label:\n",
      "torch.Size([25, 1])\n",
      "Input:\n",
      "torch.Size([25, 6])\n",
      "Label:\n",
      "torch.Size([25, 1])\n",
      "Input:\n",
      "torch.Size([25, 6])\n",
      "Label:\n",
      "torch.Size([25, 1])\n",
      "Input:\n",
      "torch.Size([25, 6])\n",
      "Label:\n",
      "torch.Size([25, 1])\n",
      "Input:\n",
      "torch.Size([25, 6])\n",
      "Label:\n",
      "torch.Size([25, 1])\n",
      "Input:\n",
      "torch.Size([25, 6])\n",
      "Label:\n",
      "torch.Size([25, 1])\n"
     ]
    }
   ],
   "source": [
    "#verify shapes for both training and testing model\n",
    "print(\"Checking the shape of training_model:\\n\")\n",
    "for i,j  in training_set:\n",
    "    print(\"Input:\")\n",
    "    print(i.shape)\n",
    "    print(\"Label:\")\n",
    "    print(j.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0996def9-e188-46b1-887c-a55ba19d10fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the shape of testing_model:\n",
      "\n",
      "Input:\n",
      "torch.Size([25, 6])\n",
      "Label:\n",
      "torch.Size([25, 1])\n",
      "Input:\n",
      "torch.Size([25, 6])\n",
      "Label:\n",
      "torch.Size([25, 1])\n",
      "Input:\n",
      "torch.Size([25, 6])\n",
      "Label:\n",
      "torch.Size([25, 1])\n",
      "Input:\n",
      "torch.Size([25, 6])\n",
      "Label:\n",
      "torch.Size([25, 1])\n",
      "Input:\n",
      "torch.Size([25, 6])\n",
      "Label:\n",
      "torch.Size([25, 1])\n",
      "Input:\n",
      "torch.Size([25, 6])\n",
      "Label:\n",
      "torch.Size([25, 1])\n",
      "Input:\n",
      "torch.Size([25, 6])\n",
      "Label:\n",
      "torch.Size([25, 1])\n",
      "Input:\n",
      "torch.Size([25, 6])\n",
      "Label:\n",
      "torch.Size([25, 1])\n",
      "Input:\n",
      "torch.Size([25, 6])\n",
      "Label:\n",
      "torch.Size([25, 1])\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking the shape of testing_model:\\n\")\n",
    "for i,j  in training_set:\n",
    "    print(\"Input:\")\n",
    "    print(i.shape)\n",
    "    print(\"Label:\")\n",
    "    print(j.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb6f08c-5e70-4b14-b62c-4686d9f7aace",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "In this task you will try to predict number of crime victims as a **real number**. Therefore the machine learning problem is a **regression** problem. \n",
    "\n",
    "* Define the proper loss function for this task\n",
    "* what should the size of the predicted output be?\n",
    "* explain your choice of architecture, including how many layers you will be using\n",
    "* define an optimizer for training this model, choose a proper learning rate \n",
    "* write a training loop that obtains a batch out of the  training data and calculates the forward and backward passes over the neural network. Call the optimizer to update the weights of the neural network.\n",
    "* write a for loop that continues the training over a number of epochs. At the end of each epoch, calculate the ``MSE`` error on the test data and print it.\n",
    "* is your model training well? Adjust the learning rate, hidden size of the network, and try different activation functions and number of layers to achieve the best accuracy and report it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e15f4d84-7a9a-41a2-9177-c114fe388963",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the predicted output size should be the same size as the actual y/model will predict 1 value\n",
    "#for every batch row\n",
    "class RegressionModel(Module):\n",
    "    #will use only 1 linear layer with the expectation that a linear regression is not a complicated enough\n",
    "    #function to require more than 1 layer\n",
    "    def __init__(self):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.linear = Linear(6, 1)\n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "        \n",
    "def model_train_regress(loader, model, loss_fn, optimizer):\n",
    "    size = len(loader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X,y) in enumerate(loader):\n",
    "        pred = model(X).float()\n",
    "        loss = loss_fn(pred, y.float())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        loss, current = loss.item(), (batch + 1) * len(X)\n",
    "        #loss for each batch\n",
    "        print(f\"MSE loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def model_test_regress(loader, model, loss_fn, y_mean, y_std):\n",
    "    #y_mean and y_std is used to inverse the normalize done for y\n",
    "    #the prediction removes the normalization and converts to actual predicted number of total people involved\n",
    "    #it is possible that some values may be loss (with negative values being generalized and rounding)\n",
    "    model.eval()\n",
    "    size = len(loader.dataset)\n",
    "    num_batches = len(loader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in loader:\n",
    "            #commented out code checks pred and y loss and accuracy if it were denormalized (predicted number); some values get loss here if used\n",
    "            #pred = (model(X)*y_std+y_mean).round()\n",
    "            #pred[pred<0] = 0\n",
    "            #y = (y*y_std+y_mean).round()\n",
    "            #y[y<0] = 0\n",
    "            pred = model(X)\n",
    "            pred = (pred*y_std+y_mean).round()\n",
    "            y_new = y\n",
    "            y_new = (y_new*y_std+y_mean).round()\n",
    "            pred[pred<0] = 0\n",
    "            y[y<0] = 0\n",
    "            test_loss += loss_fn(model(X),y).item()\n",
    "            correct += (pred == y_new).float().sum()\n",
    "            \n",
    "        test_loss /= num_batches\n",
    "        correct /= size\n",
    "        print(f\"Test Error: \\n Accuracy: {100*correct:>0.1f}%, Avg MSE loss: {test_loss:>8f} \\n\")\n",
    "        #print(f\"Test Error: \\n Avg MSE loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a79af6f-6061-41f0-90fa-ff0dbc7415eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters report\n",
    "#model stagnates around epoch 7 so any more of 7 seem to be pointless. epoch 15 is just to see for potential outlier\n",
    "epoch = 15\n",
    "#1e-1 seems to give the lowest consistent MSE error. 1e-2 also comes close as the second lowest. 1 and higher seems to be unreliable and along with \n",
    "#1e-4\n",
    "learning_rate = 1e-1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "regressModel = RegressionModel()\n",
    "loss_fn_MSE = MSELoss()\n",
    "#Loss_fn_MSE = CrossEntropyLoss()\n",
    "optimizer_regress = torch.optim.SGD(regressModel.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e6cc31d-377c-4fb2-9739-26efc9016bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing set before Trained Model:\n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.0%, Avg MSE loss: 1.391029 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_mean_testing = testing_label.mean()\n",
    "y_std_testing = testing_label.std()\n",
    "print(\"Testing set before Trained Model:\\n\")\n",
    "model_test_regress(testing_set,regressModel, loss_fn_MSE,y_mean_testing, y_std_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23fa86a6-089c-4e34-b4a4-043317e1606e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training. Printing the loss for each batch (loss slowly decreases by each enoch for each batch):\n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "MSE loss: 0.705426  [   25/  225]\n",
      "MSE loss: 0.726206  [   50/  225]\n",
      "MSE loss: 0.681002  [   75/  225]\n",
      "MSE loss: 0.403715  [  100/  225]\n",
      "MSE loss: 1.137668  [  125/  225]\n",
      "MSE loss: 1.646970  [  150/  225]\n",
      "MSE loss: 1.080604  [  175/  225]\n",
      "MSE loss: 1.907759  [  200/  225]\n",
      "MSE loss: 2.700341  [  225/  225]\n",
      "Test Error: \n",
      " Accuracy: 47.6%, Avg MSE loss: 0.846307 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "MSE loss: 0.544378  [   25/  225]\n",
      "MSE loss: 0.367576  [   50/  225]\n",
      "MSE loss: 0.775950  [   75/  225]\n",
      "MSE loss: 0.541695  [  100/  225]\n",
      "MSE loss: 0.953219  [  125/  225]\n",
      "MSE loss: 1.635067  [  150/  225]\n",
      "MSE loss: 1.021165  [  175/  225]\n",
      "MSE loss: 1.160865  [  200/  225]\n",
      "MSE loss: 2.127523  [  225/  225]\n",
      "Test Error: \n",
      " Accuracy: 41.8%, Avg MSE loss: 0.673826 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "MSE loss: 0.565327  [   25/  225]\n",
      "MSE loss: 0.350237  [   50/  225]\n",
      "MSE loss: 0.760405  [   75/  225]\n",
      "MSE loss: 0.685328  [  100/  225]\n",
      "MSE loss: 0.981694  [  125/  225]\n",
      "MSE loss: 1.719314  [  150/  225]\n",
      "MSE loss: 0.908220  [  175/  225]\n",
      "MSE loss: 1.182126  [  200/  225]\n",
      "MSE loss: 2.136992  [  225/  225]\n",
      "Test Error: \n",
      " Accuracy: 37.3%, Avg MSE loss: 0.687452 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "MSE loss: 0.525274  [   25/  225]\n",
      "MSE loss: 0.301021  [   50/  225]\n",
      "MSE loss: 0.648480  [   75/  225]\n",
      "MSE loss: 0.473317  [  100/  225]\n",
      "MSE loss: 0.971064  [  125/  225]\n",
      "MSE loss: 1.593130  [  150/  225]\n",
      "MSE loss: 0.971058  [  175/  225]\n",
      "MSE loss: 1.344695  [  200/  225]\n",
      "MSE loss: 2.260633  [  225/  225]\n",
      "Test Error: \n",
      " Accuracy: 38.2%, Avg MSE loss: 0.713914 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "MSE loss: 0.517741  [   25/  225]\n",
      "MSE loss: 0.294777  [   50/  225]\n",
      "MSE loss: 0.653739  [   75/  225]\n",
      "MSE loss: 0.439143  [  100/  225]\n",
      "MSE loss: 0.947117  [  125/  225]\n",
      "MSE loss: 1.544401  [  150/  225]\n",
      "MSE loss: 0.900381  [  175/  225]\n",
      "MSE loss: 1.290656  [  200/  225]\n",
      "MSE loss: 2.259478  [  225/  225]\n",
      "Test Error: \n",
      " Accuracy: 39.6%, Avg MSE loss: 0.698488 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "MSE loss: 0.538639  [   25/  225]\n",
      "MSE loss: 0.302620  [   50/  225]\n",
      "MSE loss: 0.683490  [   75/  225]\n",
      "MSE loss: 0.496464  [  100/  225]\n",
      "MSE loss: 0.947202  [  125/  225]\n",
      "MSE loss: 1.568972  [  150/  225]\n",
      "MSE loss: 0.922984  [  175/  225]\n",
      "MSE loss: 1.230092  [  200/  225]\n",
      "MSE loss: 2.207523  [  225/  225]\n",
      "Test Error: \n",
      " Accuracy: 38.7%, Avg MSE loss: 0.682037 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "MSE loss: 0.526731  [   25/  225]\n",
      "MSE loss: 0.302241  [   50/  225]\n",
      "MSE loss: 0.671015  [   75/  225]\n",
      "MSE loss: 0.505915  [  100/  225]\n",
      "MSE loss: 0.952916  [  125/  225]\n",
      "MSE loss: 1.595171  [  150/  225]\n",
      "MSE loss: 0.924217  [  175/  225]\n",
      "MSE loss: 1.248266  [  200/  225]\n",
      "MSE loss: 2.211316  [  225/  225]\n",
      "Test Error: \n",
      " Accuracy: 39.6%, Avg MSE loss: 0.691202 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "MSE loss: 0.521170  [   25/  225]\n",
      "MSE loss: 0.296208  [   50/  225]\n",
      "MSE loss: 0.666158  [   75/  225]\n",
      "MSE loss: 0.485082  [  100/  225]\n",
      "MSE loss: 0.951227  [  125/  225]\n",
      "MSE loss: 1.586073  [  150/  225]\n",
      "MSE loss: 0.913241  [  175/  225]\n",
      "MSE loss: 1.272511  [  200/  225]\n",
      "MSE loss: 2.232533  [  225/  225]\n",
      "Test Error: \n",
      " Accuracy: 38.7%, Avg MSE loss: 0.695052 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "MSE loss: 0.522394  [   25/  225]\n",
      "MSE loss: 0.296298  [   50/  225]\n",
      "MSE loss: 0.665462  [   75/  225]\n",
      "MSE loss: 0.477714  [  100/  225]\n",
      "MSE loss: 0.948848  [  125/  225]\n",
      "MSE loss: 1.571507  [  150/  225]\n",
      "MSE loss: 0.922509  [  175/  225]\n",
      "MSE loss: 1.264043  [  200/  225]\n",
      "MSE loss: 2.232907  [  225/  225]\n",
      "Test Error: \n",
      " Accuracy: 38.2%, Avg MSE loss: 0.690626 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "MSE loss: 0.526192  [   25/  225]\n",
      "MSE loss: 0.299320  [   50/  225]\n",
      "MSE loss: 0.670796  [   75/  225]\n",
      "MSE loss: 0.490909  [  100/  225]\n",
      "MSE loss: 0.949539  [  125/  225]\n",
      "MSE loss: 1.578846  [  150/  225]\n",
      "MSE loss: 0.916012  [  175/  225]\n",
      "MSE loss: 1.254660  [  200/  225]\n",
      "MSE loss: 2.223662  [  225/  225]\n",
      "Test Error: \n",
      " Accuracy: 39.6%, Avg MSE loss: 0.689829 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "MSE loss: 0.526147  [   25/  225]\n",
      "MSE loss: 0.298549  [   50/  225]\n",
      "MSE loss: 0.669120  [   75/  225]\n",
      "MSE loss: 0.490727  [  100/  225]\n",
      "MSE loss: 0.950263  [  125/  225]\n",
      "MSE loss: 1.580651  [  150/  225]\n",
      "MSE loss: 0.923810  [  175/  225]\n",
      "MSE loss: 1.262442  [  200/  225]\n",
      "MSE loss: 2.221628  [  225/  225]\n",
      "Test Error: \n",
      " Accuracy: 40.0%, Avg MSE loss: 0.691521 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "MSE loss: 0.524738  [   25/  225]\n",
      "MSE loss: 0.297378  [   50/  225]\n",
      "MSE loss: 0.668634  [   75/  225]\n",
      "MSE loss: 0.488266  [  100/  225]\n",
      "MSE loss: 0.949869  [  125/  225]\n",
      "MSE loss: 1.580154  [  150/  225]\n",
      "MSE loss: 0.919612  [  175/  225]\n",
      "MSE loss: 1.263371  [  200/  225]\n",
      "MSE loss: 2.226763  [  225/  225]\n",
      "Test Error: \n",
      " Accuracy: 39.1%, Avg MSE loss: 0.691802 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "MSE loss: 0.525298  [   25/  225]\n",
      "MSE loss: 0.297924  [   50/  225]\n",
      "MSE loss: 0.668981  [   75/  225]\n",
      "MSE loss: 0.487974  [  100/  225]\n",
      "MSE loss: 0.949652  [  125/  225]\n",
      "MSE loss: 1.578470  [  150/  225]\n",
      "MSE loss: 0.920984  [  175/  225]\n",
      "MSE loss: 1.261976  [  200/  225]\n",
      "MSE loss: 2.227840  [  225/  225]\n",
      "Test Error: \n",
      " Accuracy: 39.1%, Avg MSE loss: 0.690984 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "MSE loss: 0.525392  [   25/  225]\n",
      "MSE loss: 0.298440  [   50/  225]\n",
      "MSE loss: 0.669503  [   75/  225]\n",
      "MSE loss: 0.489442  [  100/  225]\n",
      "MSE loss: 0.949852  [  125/  225]\n",
      "MSE loss: 1.579389  [  150/  225]\n",
      "MSE loss: 0.920398  [  175/  225]\n",
      "MSE loss: 1.262259  [  200/  225]\n",
      "MSE loss: 2.226310  [  225/  225]\n",
      "Test Error: \n",
      " Accuracy: 39.1%, Avg MSE loss: 0.691136 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "MSE loss: 0.525300  [   25/  225]\n",
      "MSE loss: 0.298172  [   50/  225]\n",
      "MSE loss: 0.669407  [   75/  225]\n",
      "MSE loss: 0.489417  [  100/  225]\n",
      "MSE loss: 0.949876  [  125/  225]\n",
      "MSE loss: 1.579720  [  150/  225]\n",
      "MSE loss: 0.921751  [  175/  225]\n",
      "MSE loss: 1.263321  [  200/  225]\n",
      "MSE loss: 2.225956  [  225/  225]\n",
      "Test Error: \n",
      " Accuracy: 39.1%, Avg MSE loss: 0.691397 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "regressModel = RegressionModel()\n",
    "loss_fn_MSE = MSELoss()\n",
    "optimizer_regress = torch.optim.Adam(regressModel.parameters(), lr=learning_rate)\n",
    "y_mean_training = training_label.mean()\n",
    "y_std_training = training_label.std()\n",
    "print(\"Training. Printing the loss for each batch (loss slowly decreases by each enoch for each batch):\\n\")\n",
    "for t in range(epoch):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    model_train_regress(training_set, regressModel, loss_fn_MSE, optimizer_regress)\n",
    "    model_test_regress(training_set, regressModel, loss_fn_MSE, y_mean_training, y_std_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d6837fc-238e-4c61-a9b6-8ee92bc45924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing set after Trained Model:\n",
      "\n",
      "Test Error: \n",
      " Accuracy: 42.0%, Avg MSE loss: 0.819287 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_mean_testing = testing_label.mean()\n",
    "y_std_testing = testing_label.mean()\n",
    "print(\"Testing set after Trained Model:\\n\")\n",
    "model_test_regress(testing_set, regressModel, loss_fn_MSE, y_mean_testing, y_std_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5e01f86-8e80-4bfe-9536-b95f9db04d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary: The regression model trained ok initially but quickly stagnates around epoch 7. The Avg MSE loss does get generally better at predicting \n",
    "#the test set (if prediction was based off MSE loss) after training the regression model. Also during the training, the accuracy appears to go down \n",
    "#initially sometimes, but this may just be due to initial weight rounding better when denormalizing. The MSE loss does go down still despite decreasing\n",
    "#accuracy implying that the model is training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e3fc70-c6ce-4589-9930-128951290e8d",
   "metadata": {},
   "source": [
    "## Task 4 \n",
    "\n",
    "In this task, you will try to predict the number of crime victims as a **class number**. Therefore the machine learning problem is a **classification** problem. \n",
    "\n",
    "* Repeat all the steps in task 3. Specifically, pay attention to the differences with regression.\n",
    "* How would you find the number of classes on the output data?\n",
    "* How is the architecture different?\n",
    "* How is the loss function different?\n",
    "* Calculate the Accuracy for test data as the number of correct classified outputs divided by the total number of test data in each epoch. Report it at the end of each epoch\n",
    "* Try a few variations of learning rate, hidden dimensions, layers, etc. What is the best accuracy that you can get? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e204445f-a39a-4dcb-b35d-306671d1fbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.)\n",
      "tensor(0.)\n",
      "\n",
      "tensor(6.)\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "#to get the number of classes, I can get the range of the actual output data and have that range be the classes\n",
    "#if the range is not too large\n",
    "\n",
    "#checking the range\n",
    "print(training_label.max())\n",
    "print(training_label.min())\n",
    "print()\n",
    "print(testing_label.max())\n",
    "print(testing_label.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e9864a6-ecea-4a18-a4a2-6d74d3aa380a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#since the range is not too large (0:4 or 0:6), I can use this as for the number of classes\n",
    "#using only the range of the training_label (since testing acts as being unpredicatable), I can have classes be from 0:4 with an/some extra class(es) \n",
    "#acting for outliers if the predicted value is even higher than the expected max. Like in this case, 4 is expected max but testing label has 6 as the \n",
    "#max/unconsidered clas\n",
    "#for this test, the outlier class will be to double the expected max to handle outliers (from 0:4 to 0:8).\n",
    "#also note that outliers for the minimum is not considered as having negative people on the scene is unrealistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d929c52-af34-4081-92cd-3463a3fc4db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModel(Module):\n",
    "    #this function is more complicated than the RegressionModel; more layers will be used\n",
    "    #will use 3 layers with arbitrary hidden nodes value as an arbitrary start\n",
    "    #output between a classification of range 0:8\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = Sequential(\n",
    "            Linear(6, 7),\n",
    "            ReLU(),\n",
    "            Linear(7, 7),\n",
    "            #ReLU(),\n",
    "            Linear(7,8))\n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear_relu_stack(x)\n",
    "        return y_pred\n",
    "\n",
    "def model_train_classify(loader, model, loss_fn, optimizer, y_mean, y_std):\n",
    "    #y_mean and y_std to denormalize y. not used in regression model_train\n",
    "    size = len(loader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X,y) in enumerate(loader):\n",
    "        #argmax(1) gets the predicted/highest class of each batch\n",
    "        pred = model(X).float()\n",
    "        #denormalize y\n",
    "        y = (y*y_std+y_mean).round()\n",
    "        y[y<0] = 0\n",
    "        #flatten to fit description of target of cross entropy loss\n",
    "        y = y.flatten().long()\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        loss, current = loss.item(), (batch + 1) * len(X)\n",
    "        #loss for each batch\n",
    "        #print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def model_test_classify(loader, model, loss_fn, y_mean, y_std):\n",
    "    #y_mean and y_std to denormalize y\n",
    "    model.eval()\n",
    "    size = len(loader.dataset)\n",
    "    num_batches = len(loader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in loader:\n",
    "            pred = model(X).argmax(1).reshape((25,1)).float()\n",
    "            y = (y*y_std+y_mean).round()\n",
    "            y[y<0] = 0\n",
    "            correct += (pred == y).float().sum()\n",
    "        test_loss /= num_batches\n",
    "        correct /= size\n",
    "        print(f\"Test Error: \\n Accuracy: {100*correct:>0.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16df7458-ec05-4b4e-878d-7c1b911c3a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this before training\n",
    "#hyperparameters report\n",
    "#around 550 does the max seem to stagnate\n",
    "epoch = 550\n",
    "#learning_rate 1e-3 appears to give the greatest accuracy without overfitting. learning_rate 1e-2 actually gives the highest accuracy for the training\n",
    "#set but in the testing set, it gives less accuracy than 1e-3. 1e-3 gives a consistent value around 45 but 1e-2 goes lower in the testing set.\n",
    "learning_rate = 1e-3\n",
    "\n",
    "classifyModel = ClassificationModel()\n",
    "#main difference between CrossEntropyLoss and MSE is that CrossEntropyLoss is nonlinear and MSE is linear. This is due to some use of some activation \n",
    "#function (ReLU) that generalizes values to turn on or off to distinguish between classes. The use of the activation function in the model makes \n",
    "#CrossEntropyLoss more ideal. The loss function will now require input be the probabilities of all the classes for all batches and y will be flatten\n",
    "#for the sake of the intended shape to be taken by the function.\n",
    "loss_fn_cross = CrossEntropyLoss()\n",
    "optimizer_classify = torch.optim.Adam(classifyModel.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f58d8a2-5cad-46fe-8b59-ec94401e56d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing set before Trained Model:\n",
      "\n",
      "Test Error: \n",
      " Accuracy: 6.0%\n"
     ]
    }
   ],
   "source": [
    "y_mean_testing = testing_label.mean()\n",
    "y_std_testing = testing_label.std()\n",
    "print(\"Testing set before Trained Model:\\n\")\n",
    "model_test_classify(testing_set, classifyModel, loss_fn_cross, y_mean_testing, y_std_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "244bec1e-64b6-4b2c-8c28-3bbae681ce93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training. Printing the loss for each batch (loss slowly decreases by each enoch for each batch):\n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.4%\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.4%\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 1.3%\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 28.0%\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 36.4%\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 40.0%\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 44.4%\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 48.9%\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 50.7%\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 51.1%\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 50.2%\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 50.2%\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 50.2%\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 50.2%\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 50.7%\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 50.7%\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 50.7%\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 50.7%\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 50.7%\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 50.7%\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 50.7%\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 50.7%\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 50.7%\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 51.6%\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 51.6%\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 51.6%\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 51.6%\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 52.0%\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 52.0%\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 52.0%\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 52.0%\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 52.0%\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 52.0%\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 52.0%\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 52.0%\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 52.0%\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 52.0%\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 52.0%\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 52.0%\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 51.6%\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 51.6%\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 51.6%\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 51.6%\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 51.6%\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 51.1%\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 51.1%\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 51.1%\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 51.1%\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 51.1%\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 51.1%\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 51.1%\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 51.1%\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 51.1%\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 51.1%\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 51.1%\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 51.1%\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 51.1%\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 51.1%\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 51.1%\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 51.1%\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 51.1%\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 51.1%\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 51.1%\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 51.1%\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 51.1%\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 51.1%\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 51.6%\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 51.6%\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 51.1%\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 51.1%\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 51.1%\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 51.1%\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 51.1%\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 51.1%\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 52.0%\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 52.0%\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 53.3%\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 53.3%\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 53.3%\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 52.9%\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 53.3%\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 52.9%\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 52.9%\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 52.9%\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 52.9%\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 52.9%\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 53.3%\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 53.8%\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 53.8%\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 53.8%\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 54.2%\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 54.7%\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.1%\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.1%\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.1%\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.1%\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.6%\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 56.0%\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 56.0%\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 56.4%\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 56.4%\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 56.9%\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 56.9%\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 57.3%\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 56.9%\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 56.9%\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 56.4%\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 56.9%\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 56.9%\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 57.3%\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 57.3%\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 57.3%\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 58.2%\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 58.7%\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 58.7%\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 59.1%\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 59.1%\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 58.7%\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 59.6%\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 59.6%\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 58.7%\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 58.7%\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 58.2%\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 58.2%\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 58.2%\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 58.2%\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 59.6%\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 59.1%\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 59.1%\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 59.6%\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 60.4%\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 60.0%\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 60.0%\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 60.0%\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 60.4%\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 60.9%\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 61.3%\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 61.3%\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 61.3%\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 61.3%\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 61.3%\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 61.8%\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 61.8%\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 61.8%\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 61.8%\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 61.8%\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 61.8%\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 61.3%\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 61.3%\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 61.8%\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 61.8%\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 61.3%\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 61.3%\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 61.3%\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 61.3%\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 61.3%\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 61.3%\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 60.9%\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 60.9%\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 60.9%\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 61.3%\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 61.3%\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 61.3%\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 61.8%\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.7%\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.7%\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.7%\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.7%\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.7%\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.7%\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.7%\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.7%\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.7%\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.1%\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.1%\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.1%\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.1%\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.1%\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.1%\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.1%\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.1%\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.1%\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.7%\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.7%\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.7%\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.7%\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.7%\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.1%\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.1%\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.1%\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.1%\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.1%\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.1%\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 299\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 302\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 303\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 304\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 305\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 306\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 307\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 308\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 309\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 310\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 311\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 312\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 313\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 314\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 315\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 316\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 317\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 318\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 319\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 320\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 321\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 322\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 323\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 324\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 325\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 326\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 327\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 328\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 329\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 330\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 331\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 332\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 333\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 334\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 335\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 336\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 337\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 338\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 339\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 340\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 341\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 342\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 343\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 344\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 345\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 346\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 347\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 348\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 349\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 351\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 352\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 353\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 354\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 355\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 356\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 357\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 358\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 359\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 360\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 361\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 362\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.1%\n",
      "Epoch 363\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.1%\n",
      "Epoch 364\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.1%\n",
      "Epoch 365\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.1%\n",
      "Epoch 366\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.1%\n",
      "Epoch 367\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.1%\n",
      "Epoch 368\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.1%\n",
      "Epoch 369\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.1%\n",
      "Epoch 370\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.1%\n",
      "Epoch 371\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.1%\n",
      "Epoch 372\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.1%\n",
      "Epoch 373\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.1%\n",
      "Epoch 374\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.1%\n",
      "Epoch 375\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.1%\n",
      "Epoch 376\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.1%\n",
      "Epoch 377\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.1%\n",
      "Epoch 378\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.1%\n",
      "Epoch 379\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.1%\n",
      "Epoch 380\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.1%\n",
      "Epoch 381\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.1%\n",
      "Epoch 382\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 383\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.1%\n",
      "Epoch 384\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.1%\n",
      "Epoch 385\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.1%\n",
      "Epoch 386\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 387\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.1%\n",
      "Epoch 388\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.1%\n",
      "Epoch 389\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 390\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 391\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 392\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 393\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 394\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 395\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 396\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 397\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 398\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 399\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 401\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 402\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 403\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 404\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 405\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 406\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 407\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 408\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 409\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 410\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 411\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 412\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 413\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 414\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 415\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 416\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 417\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 418\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 419\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 420\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 421\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 422\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.6%\n",
      "Epoch 423\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 424\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 425\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 426\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 427\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 428\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 429\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 430\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 431\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%\n",
      "Epoch 432\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 433\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 434\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 435\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 436\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 437\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 438\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 439\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 440\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 441\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 442\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 443\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 444\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 445\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 446\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 447\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 448\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 449\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 451\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 452\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 453\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 454\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 455\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 456\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 457\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 458\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 459\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 460\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 461\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 462\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 463\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 464\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 465\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 466\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 467\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 468\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 469\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 470\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 471\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 472\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 473\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 474\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 475\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 476\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 477\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 478\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 479\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 480\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 481\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 482\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 483\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 484\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 485\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 486\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 487\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 488\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 489\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 490\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 491\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 492\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 493\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 494\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 495\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 496\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 497\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 498\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 499\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 501\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 502\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 503\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 504\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 505\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 506\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 507\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 508\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 509\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 510\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 511\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 512\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 513\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 514\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 515\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 516\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 517\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 518\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 519\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 520\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 521\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 522\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 523\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 524\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 525\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 526\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 527\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 528\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 529\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 530\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 531\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 532\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 533\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 534\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 535\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 536\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 537\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 538\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 539\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 540\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 541\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 542\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 543\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 544\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 545\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 546\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 547\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 548\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 549\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n",
      "Epoch 550\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.4%\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "\n",
    "\n",
    "y_mean_training = training_label.mean()\n",
    "y_std_training = training_label.std()\n",
    "print(\"Training. Printing the loss for each batch (loss slowly decreases by each enoch for each batch):\\n\")\n",
    "\n",
    "for t in range(epoch):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    model_train_classify(training_set, classifyModel, loss_fn_cross, optimizer_classify,y_mean_training, y_std_training)\n",
    "    model_test_classify(training_set, classifyModel, loss_fn_cross, y_mean_training, y_std_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff47edda-fc83-401a-a7c8-6ea3469da4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing set after Trained Model:\n",
      "\n",
      "Test Error: \n",
      " Accuracy: 46.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing set after Trained Model:\\n\")\n",
    "model_test_classify(testing_set, classifyModel, loss_fn_cross,y_mean_testing,y_std_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d2a304-6197-4cd9-b31e-745f7862f213",
   "metadata": {},
   "source": [
    "## Task 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e4ef16-d828-45e5-bd58-1c49fcfecf52",
   "metadata": {},
   "source": [
    "### Reflect on your results\n",
    "\n",
    "* Write a paragraph about your experience with tasks 3 and 4. How do you compare the results? Which one worked better? Why?\n",
    "* Write a piece of code that finds an example of a  miss-classification. Calculate the probabilities for the output classes and plot them in a bar chart. Also, indicate what is the correct class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ade8e21-522e-4453-8f16-d5438384b6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Regression Model on Testing set:\n",
      "Test Error: \n",
      " Accuracy: 46.0%, Avg MSE loss: 0.819287 \n",
      "\n",
      "Trained Classify Model on Testing set:\n",
      "Test Error: \n",
      " Accuracy: 46.0%\n"
     ]
    }
   ],
   "source": [
    "#ensure that the regress and classify model are trained first\n",
    "print(\"Trained Regression Model on Testing set:\")\n",
    "model_test_regress(testing_set, regressModel, loss_fn_MSE, y_mean_testing, y_std_testing)\n",
    "print(\"Trained Classify Model on Testing set:\")\n",
    "model_test_classify(testing_set, classifyModel, loss_fn_cross,y_mean_testing,y_std_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cbe3ff32-8d93-4903-bc9a-386ec89d6e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To compare results of the models, I get the prediction of both trained model and denormalize them and compare with the actual labels (that are also \n",
    "#denormalize). After trying the above cells a couple times (All the cells to recreate and train the regress and classify models), it appears that \n",
    "#the regression model consistently gets 46% accuracy while the classify model can get between 42% and 50% accuracy. Because one model appears consistent\n",
    "#(but not by much) and the other has more upside but less consistent, the discussion of which is better seems to be by situation. I am not sure which \n",
    "#model worked better but the classify model was capable of handling more epoch than the regress model which stagnated very early. Having the model \n",
    "#stagnate so quick could be a sign of a flaw in my regression model; the classify model acted more as expected for how training should be.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ce02fee-66ea-4517-9162-9db30a7146f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_one_miss_classification(loader, model, y_mean, y_std):\n",
    "    '''finds a single miss classification and creates a bar graph showing the probabilities of its classes'''\n",
    "    model.eval()\n",
    "    for X, y in loader:\n",
    "        pred = model(X)\n",
    "        pred_max = pred.argmax(1).reshape((25,1)).float()\n",
    "        y_denorm = y\n",
    "        y_denorm = (y_denorm*y_std+y_mean).round()\n",
    "        y_denorm[y_denorm < 0] = 0\n",
    "        for i in range(0, len(pred)):\n",
    "            if not (pred_max[i] == y_denorm[i]):\n",
    "                #abs() as sometimes denormalization converts to -0 instead of 0. Use of abs should not affect class names as its all positive/no \n",
    "                #negative anyway\n",
    "                print(f\"Predicted: {pred_max[i].item()}, Actual: {abs(y_denorm[i].item())}\")\n",
    "                #convert logit to prob with softmax\n",
    "                prob = torch.softmax(pred[i], dim=0) * 100\n",
    "                plt.xlabel(\"Class\")\n",
    "                plt.ylabel(\"Likelihood\")\n",
    "                bar = plt.bar([i for i in range(8)],prob.detach())\n",
    "                #return as job is already done\n",
    "                return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8aafb929-509c-46f8-88a6-44ddaad9cee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 0.0, Actual: 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "find_one_miss_classification(testing_set, classifyModel, y_mean_testing, y_std_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b000b0b-fa37-4f8d-8ec7-0a1e6e749424",
   "metadata": {},
   "source": [
    "## Task 6: Exploring the patterns in raw data\n",
    "\n",
    "* Plot the crime incidents as a `scatter` plot using the coordinates. Use the color property of each datapoint to indicate the day of the week. Is there a pattern in the plot?\n",
    "* Now make a new scatter plot and use the color property of each datapoint to indicate the number of persons involved in the incident. Is there a pattern here?\n",
    "* use numpy (or pandas if you like) to sort the number of crimes reported by the day of the week. What days are most frequent?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5b396aaf-5d1a-49cd-ae1d-b63af7e561f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Friday', 'Thursday', 'Tuesday', 'Saturday', 'Sunday', 'Monday',\n",
      "       'Wednesday'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAg4AAAGiCAYAAACPjU4xAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABGHUlEQVR4nO3deXxU1f3/8ddMlklCMkkgG0vYXFhEREEx4AKIRL5xoVqtiApCsWJcEKqQyk/UFuNCFVtxV9Baq1LrhgqETbQEwdggBEEQNBGZsCaBmHXm/v4YMjIkgQmZ5GbC+/l43AfMveeee07QuZ98zrnnWgzDMBARERHxgdXsBoiIiEjgUOAgIiIiPlPgICIiIj5T4CAiIiI+U+AgIiIiPlPgICIiIj5T4CAiIiI+U+AgIiIiPlPgICIiIj5T4CAiIiI+U+AgIiJyEtm5cyc33ngj7dq1Izw8nDPPPJOvvvrK5/ODm7BtIiIi0oIcOHCAwYMHM3ToUD799FPi4+PZunUrsbGxPtdh0UuuRERETg7Tp0/nv//9L59//vkJ13FSBA4ul4uff/6ZqKgoLBaL2c0REZEWzDAMDh48SIcOHbBam2ZEv7y8nMrKSr/UZRhGrXubzWbDZrPVKtu7d29SU1P56aef+Oyzz+jYsSO33347EydObNAFW72CggID0KZNmzZt2nzeCgoKmuSeVFZW5td2RkZG1to3c+bMOq9ts9kMm81mZGRkGF9//bXxwgsvGGFhYcb8+fN9bv9JkXEoLi4mJiaGgoIC7Ha72c0REZEWrKSkhOTkZIqKioiOjm6S+v1d79H3t/oyDqGhoQwYMIDVq1d79t11112sW7eO7Oxsn651UkyOrEnh2O12BQ4iIuKT5hjabuw1an739/X+1r59e3r37u21r1evXrz77rs+X/OkCBxERERaGovF4pfgpCEDB4MHD2bLli1e+7777ju6dOnicx0KHERERExgRuBwzz33MGjQIB555BGuu+461q5dy4svvsiLL77ocx0KHERERExgtVr9MlThcrl8Ln/uuefy3nvvkZGRwcMPP0y3bt2YM2cOY8aM8bmOFrVy5KOPPorFYmHy5MmefeXl5aSnp9OuXTsiIyO55pprKCwsNK+RIiIiAezyyy9nw4YNlJeX8+233zbsUUxaUOCwbt06XnjhBfr27eu1/5577uGjjz5iwYIFfPbZZ/z8889cffXVJrVSRETEP2qGKhq7NbcWETgcOnSIMWPG8NJLL3kte1lcXMwrr7zCk08+ybBhw+jfvz/z5s1j9erVrFmzxsQWi4iINI4Ch0ZIT08nLS2N4cOHe+3PycmhqqrKa3/Pnj3p3LnzMZ83raiooKSkxGsTERGRxjN9cuRbb73F119/zbp162odczgchIaGEhMT47U/MTERh8NRb52ZmZk89NBD/m6qiIiI35iVMWgsUzMOBQUF3H333fzzn/8kLCzMb/VmZGRQXFzs2QoKChpdZ04OvP46vPsuFBf7oZEiInJSC9ShClMzDjk5OezevZtzzjnHs8/pdLJq1SqeeeYZFi9eTGVlJUVFRV5Zh8LCQpKSkuqtt76lNk/EN9/AhAkuvvrq1xirTRuDu+6y8Oc/Q1CQXy4jIiISEEwNHC655BI2bNjgte+WW26hZ8+eTJs2jeTkZEJCQli2bBnXXHMNAFu2bCE/P5+UlJQmb9/WrTBkiEFcXDlPPPEjAwYUc+BACO+/n8hjj3XkwAF47rnASzPJyWXnTpgzB956y0lpqYX27a3ceSekpsLf/w6ffgqFhQYWC0RGQlWVBYsFunWDM8749XhlZTXurwwnYDlqq2F4fbbZoLoanM5fSwQFgcsFFot7i4iAqiqoqADf17GpKfjrtSwWCA2F8HD39Q4ebOhPyi0kxN2m006D666Djh2hsBA2bYLsbNi7F6KjYcgQmDkTOnQ4seuIBOpQRYt7ydWQIUPo168fc+bMAWDSpEl88sknzJ8/H7vdzp133gng9YKO46l5oUhxcXGD3lUxdqzB4sVVvP56LlFRTq9jCxYk8dRT3diyBU4/3ecqRZrVihVw2WUuKisNwAWsB9oB3QAnFgsYRk3azCA+3sm+fUFYLOB0WqgJBCyWUgwjHIulkuBgqKr6dWgxJMTAMAyqq620b++kXTsXmzcH43KBy+V+3a9hWGjTxh24AFgsTgwjhOBgF9XVVtq0cS9gU1pqJSjIwOl0AUEEB5fjdIZgGFZCQ3+hqir8iODCO2ixWl0Yhovw8FJ++SXqiDLlWCw2DMNy+LpBuIMfcAdCPwDFQJ/D/bUSEVFCRUUbnM5gTz0Wi4FhWIiLM+jQAbZssVBVZfDQQxZmzPDDP5a0GCd6z2ho/W3atPHLAlClpaVN1ta6tIinKo7lqaee4vLLL+eaa67hoosuIikpif/85z9Nft3ycnj7bfjNb3bVChoArryykOjoav7xjyZvisgJ2bsX0tKcVFYC/AvoCJwL1CwtayUp6Xsuu+w5rr32z1x88T+prj5ARISTxEQnNUFDTMynGEY4bdqsxzDCGDnSYMmS/WzfvpsPPzzAoEGVOJ0WMjIO8b//7WPZsgPk5u7lmmvKAQuGAf36VVBe7qJTp6fp128offsOJyxsL4mJLv71ryK2bdvLtm17eeutIpKSXNhs1QwY8D7V1WH06JHNmDEzGD/+j9x883TOPnsJYGXAgGI++ugrVq3K5sknN9G9exkhIdVceeVTpKS8C1hp1+4DwIZhWOnY8VsMI5jg4GrcgYMDuBB3ENUP6AJ8gsXiZOTIZxk37l5SUv6NxeICDGw2eOMN2LXLwvr1FhwOuOceC//v/7n3S9MoLYX8fDh0qP4yhgFr18KHH8JXXzUkcyUnosVlHJrCiUSPhYWQlASPPbaZCy88UGeZ3/++L4MGteGll/zZWhH/ePxxmDatGsgDzsGdcYgFdhIUBJ07f8uIES9jsfz6FVBREc7772fQoUM0eXlW7PZ1VFR0xOm0ERRk4//+z8Zzzx3kyF+SnE647roY9u61snLlfs8xlwsuvTSGTZtCcLkMunZ9iHbtPgagsPB6du2awurV++na1Xu53B9/tJKS0paQkFI6dNjKpZe+xNG/lK1efQ1bt17MRx99TURETbYiiBtu6EtMzHoKC7sQFLQBlyuc0tIUTjllDfn553DqqcFs2BByuJYBQM5RP7UQYAunnLKHSy+dB0Bu7qWsWfMbMjMtTJ/uXdowYNQo2LABvv+eWu2UE/fdd/DQQwYLFriHz0JCDK65BmbOtNCz56/lPv4Y/vhH2Lz5131nnAF//at7OO5ENFfGITIy0i8Zh0OHDinj0BLExEB4uMG2bRF1Hi8vt1JQEEbHjs3bLhFfLV3qTvfDy7iDBoDfAiE4neGcd96HXkEDgM1WRt++i8jLCwGCadv2Uyork4iM/IaKihgmT/6l1s0xKAjuuquUzZuD+eabX6dNWa0wdmw5LpcFsGK3/zq8WFw8ktTUilpBA0CXLi7OP7+S8vIozjxzeZ034759l1NWFsLq1b8uGNemjZPf/raQ7dsHUFKSRNu2H1NcPIjq6iDatfuJsrJwfv7ZQlBQEe6A4eigAaAKeIkdO87y/Nbaq9fnWK0uQkNrl7ZY4PbbYccO+PbbOqqTE7JhA5x/vsEXX7j4058O8dZbRcyYcYjsbBfnn2+Qm+su98EHcOWV0KULLF/u/oUvK8v9S19aGixaZGo3jitQn6pQ4FAPmw1Gj4b330+iqKj2HNJ3303i0CErN91kQuNEfOCekGgB9hyxNwmLpZzQ0FJiY+t+50ti4g5q5g5YLO5hOsNwP6XUo0ftYTuAnj3d+x0O76+UuLhfAwPD+PXO63JFk5xcf7IzOtr9Z1TUvjqPt2lzAIvFRUmJ9/+bXbqU4XS652xYrWXUfMUFBVUDUFFhYLWW4/0zOdpuXC73nAgAm62ckJByysrqLp2Q4P6ztPQYVUqD/OEPBklJTpYu3c/tt5cxdGglt91WxrJl+0lOrubWWw2cTrjnHhg5Ej75BIYOdf9bDB8OixfDsGEweXLLHrZQ4NAK/b//Z8EwQpg0qQ9ZWe0oKgpmx45w5szpyty5XZg82cIpp5jdSpG6XXyxFagELj1i788YRjhVVWGUlbWp87yDB9sd/ls1RUXDCAoq4pdfTgNg8+a6nz+u2Z+U5J1B+OyzUIKCnIALi+XXO2tIyI98+WX9Xz8//uj+Miws7Fbn8T17uhyet1DutX/r1jaEhFQSFFTOwYPnEBq6H4Dy8kgAwsOtOJ0RQApQd/9hBNHRO7Fa3XecAwcSqahoQ0hI3aWXLnU/iaHvAv/YsAGysy3cd98hoqO97/pRUQb33VfKunUWXnnFnemZMcOd3TpSUBD86U+wZQvo7QT+p8DhGLp2hf/+10LXrmHMnHk6//d/5zJmTD+yspKYNQtmzza7hSL1mzgRgoKCgRuBQYf3LgDcN9uNG4fUOscwLGzYcAk2WzVg5cCBoURGfk1VVXtCQsqYMyei1m9wTic8/XQbevaspm/fas/+//0vmDfftB3OAFjYu/d3nmPt2v2H3NwwFi2qnf9fvDiUvDwbMTG7+PrrkVRVea/J4nRaWbfuShITyxgw4NfV2PbvD+bddxM47bQ19Oixlj17biA6Ogur1cl33w0mMfFHwMDlsgORwKw6fmpDgavp0+e/ALhcFr78chQWi5PFiw0qKrxL79gBjz1mcO210LZtHdVJg9XMVRg8uKrO4xdcUAnA//7n/tyvX9311Oz/6Sf/tc3fAjXjYPqS0y3d6afDypUWtmxxP8cdEQEXXWQhPNzslokcW/v28M47Vn772yAMYwXwOrAI+BjDuI6cnDQsFujTZyVhYaUUFcWzbt0V/PRTD9q0ceJyOamqCqKo6DKs1kNUVUXy/vtQXW3hjjt+4dRTnWzcGMxf/9qG1atD6Ny5mn/8I4y4OBeffRZ6OGgwgGp69TL49ttJVFR0Ii7uP9hs24mI+I7x40/jllvKuOqqCiwWgw8+COPVV8MJDq4iOXkj3357Ee++O41+/bKIi8unuDiR3Nzh7NnTlUGDDrB9ewSxsVWsWxfNSy8lU17uIjk5j+LiBCCUPXtGERpawsGDsZSXh+N0GlitzsPzLu4G+gOv4n4c83LgJqKi9tKuXT5btw5gw4Zh7N7dhfDwUlaujOTMMw3uuMNC167uNR2efdYgMtLCX/9qyj9xqxTpTg6xe7eV6OjaQ2N79rizW4mJ7s/ffgtnn127npoApKZcS6R1HFqwpp4hK9KSbdgAf/gDrF1bhdPpzreHhzspK6uZx2AQElJBZWXEEescgPtxTOOIvwNYCAqyHF7jwc297oIFq9WFy1Uzp8CF02kF3ItG1ZSp+ezmOlwfnvp+LVcz5GE9/Pdfk6M1bfy1bM1+F4ZRU+7IBaLc13Svw3DkAlWuw+Vq2lN1+O+G53pH/zysVg4HHRAaajBqlIVnnoH4+Pp++tJQZWXQsaPBNdeUMWtW7WcwH3wwkjffDOfHHy307g0XXgj/+pf3Ey2GAVdfDbm5sG1bw1f4ba6nKmJiYvzyVEVRUVGz3t+UcRBp5c48E1avBpcrhOJi98TfiIggfvgBnn4aVq0CpzOCPn0gLS2IuXPdq02ecoqF0aMtREW5V2Hs0AEuuMA9c/2dd+DHH8FuhwEDLJSXw+efW/nf/8DhgKoqKx06QOfOwezaBWDBbofExGCqq2H7dgArISHulR5/+MF9XnW1hbZtISjISmmpe0VJw7BitbrnEVRWQnm5+y7gdFqIiHC3oU0biIuzEh7u/hwdbSE+HkpKoGPHYC69FPbutbB4sXv9hdJS2LYtiMJCCA52T6rr0yeE5GTo3NlCZSWcfz506hREbq67zIABFqxW9wqXwcHQvr2l3nkPcuLCw+Heey386U8RxMe7mDixjDZtDEpLYd68CJ5/PpyHHrIQHQ2PPgo33+wODKZPdz+GuWEDzJoF77/vXounJb8WwGKxYD16gkYDuVy1n0xqaso4iIhIi2IY7kBg9myDiAjo3NlJfn4Qv/wCkydbeOKJXydEvv46TJvmDjxrdOjgnoM2evSJXb+5Mg5t27b1S+Cwf//+Zr2/KXAQEZEWKT/fvSrnzz+75+zceKN7zYajVVXBkiWwa5f73SLDh9OobJACh2PTUIWIiLRInTu7H6s8npAQ94JPgcYfkyP1VIWIiMhJIlADB63jICIiIj5TxkFERMQEgZpxUOAgIiJiAgUOIiIi4jOr1dropyrMEHgtFhEREdMo4yAiImICDVWIiIiIzwI1cNBQhYiIiPhMGQcRERETBGrGQYGDiIiICQI1cNBQhYiIiPhMGQcRERETBGrGQYGDiIiICbQAlIiIiLR6yjiIiIiYQEMVIiIi4jMFDiIiIuKzQA0cNMdBREREfKaMg4iIiEnMyBg0lgIHERERE/jjcUzDMPzUGt9pqEJERER8poyDiIiICTQ58gRlZmZy7rnnEhUVRUJCAqNGjWLLli1eZcrLy0lPT6ddu3ZERkZyzTXXUFhYaFKLRUREGq8mcGjs1txMDxw+++wz0tPTWbNmDVlZWVRVVTFixAhKS0s9Ze655x4++ugjFixYwGeffcbPP//M1VdfbWKrRURETk6mD1UsWrTI6/P8+fNJSEggJyeHiy66iOLiYl555RXefPNNhg0bBsC8efPo1asXa9as4fzzzzej2SIiIo2iyZF+UlxcDEDbtm0ByMnJoaqqiuHDh3vK9OzZk86dO5OdnV1nHRUVFZSUlHhtIiIiLYmGKvzA5XIxefJkBg8eTJ8+fQBwOByEhoYSExPjVTYxMRGHw1FnPZmZmURHR3u25OTkpm66iIjISaFFBQ7p6els3LiRt956q1H1ZGRkUFxc7NkKCgr81EIRERH/qBmqaOzW3Eyf41DjjjvuYOHChaxatYpOnTp59iclJVFZWUlRUZFX1qGwsJCkpKQ667LZbNhstqZusoiIyAnT45gnyDAM7rjjDt577z2WL19Ot27dvI7379+fkJAQli1b5tm3ZcsW8vPzSUlJae7mioiI+IXmOJyg9PR03njjDd58802ioqJwOBw4HA7KysoAiI6OZsKECUyZMoUVK1aQk5PDLbfcQkpKip6oEBERaYAHH3ywVuDRs2fPBtVh+lDFc889B8CQIUO89s+bN49x48YB8NRTT2G1WrnmmmuoqKggNTWVZ599tplbKiIi4j9mPY55xhlnsHTpUs/n4OCGhQKmBw6+dDosLIy5c+cyd+7cZmiRiIhI0/PnHIejlx041ly/4ODgeucI+sL0oQoRERFpnOTkZK9lCDIzM+stu3XrVjp06ED37t0ZM2YM+fn5DbqW6RkHERGRk5E/hyoKCgqw2+2e/fVlGwYOHMj8+fPp0aMHu3bt4qGHHuLCCy9k48aNREVF+XRNBQ4iIiIm8OdQhd1u9woc6jNy5EjP3/v27cvAgQPp0qUL77zzDhMmTPDpmhqqEBEROUnFxMRw+umns23bNp/PUeAgIiJiAovF0uhVIxubsTh06BDff/897du39/kcBQ4iIiImMGMBqD/+8Y989tln/PDDD6xevZrf/OY3BAUFMXr0aJ/r0BwHERGRk8RPP/3E6NGj2bdvH/Hx8VxwwQWsWbOG+Ph4n+tQ4CAiImICM95V0diXSIICBxEREVP443HMk/rtmCIiIicTvR1TREREWj1lHEREREygoQoRERHxmYYqREREpNVTxkFERMQEgZpxUOAgIiJigkCd46ChChEREfGZMg4iIiIm0FCFiIiI+ExDFSIiItLqKeMgIiJiAg1ViIiIiM8UOIiIiIjPLBZLo+coaOVIERERadGUcRARETGBhipERETEZ3ocU0RERFo9ZRxERERMoKEKERER8ZmGKkRERKTVU8ZBRETEBBqqEBEREZ8FauCgoQoRERHxWcAEDnPnzqVr166EhYUxcOBA1q5da3aTRERETlhNxqGxW3MLiMDh7bffZsqUKcycOZOvv/6as846i9TUVHbv3m1200RERE6IAocm9OSTTzJx4kRuueUWevfuzfPPP09ERASvvvqq2U0TERE5IQocmkhlZSU5OTkMHz7cs89qtTJ8+HCys7PrPKeiooKSkhKvTURERBqvxQcOe/fuxel0kpiY6LU/MTERh8NR5zmZmZlER0d7tuTk5OZoqoiIiM+UcWhBMjIyKC4u9mwFBQVmN0lERMRLoAYOLX4dh7i4OIKCgigsLPTaX1hYSFJSUp3n2Gw2bDZbczRPRETkpNLiMw6hoaH079+fZcuWefa5XC6WLVtGSkqKiS0TERE5cco4NKEpU6YwduxYBgwYwHnnncecOXMoLS3llltuMbtpIiIiJyRQX3IVEIHD7373O/bs2cMDDzyAw+GgX79+LFq0qNaESREREWlaARE4ANxxxx3ccccdZjdDRETELwL1XRUBEziIiIi0Nmbc+BurxU+OFBERkZZDGQcRERETaKhCREREfKbAQURERHwWqIGD5jiIiIiIz5RxEBERMUGgZhwUOIiIiJggUAMHDVWIiIiIz5RxEBERMUGgZhwUOIiIiJggUAMHDVWIiIiIz5RxEBERMUGgZhwUOIiIiJggUAMHDVWIiIichB599FEsFguTJ09u0HnKOIiIiJjAzIzDunXreOGFF+jbt2+Dz1XGQURExAQ1gUNjt4Y6dOgQY8aM4aWXXiI2NrbB5ytwEBERMYE/A4eSkhKvraKiot7rpqenk5aWxvDhw0+o3QocREREAlxycjLR0dGeLTMzs85yb731Fl9//XW9x32hOQ4iIiIm8Occh4KCAux2u2e/zWarVbagoIC7776brKwswsLCTviaChxERERM4M/AwW63ewUOdcnJyWH37t2cc845nn1Op5NVq1bxzDPPUFFRQVBQ0HGvqcBBRETkJHDJJZewYcMGr3233HILPXv2ZNq0aT4FDaDAQURExBTN/ThmVFQUffr08drXpk0b2rVrV2v/sShwEBERMUGgrhypwEFEROQktXLlygafo8BBRETEBMo4iIiISIOYceNvLC0AJSIiIj5TxkFERMQEGqoQERERnylwEBEREZ8FauCgOQ4iIiLiM2UcRERETKCMQwP98MMPTJgwgW7duhEeHs4pp5zCzJkzqays9Cr3zTffcOGFFxIWFkZycjKPP/64SS0WERHxn5rAobFbczMt47B582ZcLhcvvPACp556Khs3bmTixImUlpYye/ZsAEpKShgxYgTDhw/n+eefZ8OGDYwfP56YmBhuvfVWs5ouIiJy0jItcLjsssu47LLLPJ+7d+/Oli1beO655zyBwz//+U8qKyt59dVXCQ0N5YwzziA3N5cnn3xSgYOIiAQ0q9WK1dq4xH9jzz+hazb7FY+huLiYtm3bej5nZ2dz0UUXERoa6tmXmprKli1bOHDgQL31VFRUUFJS4rWJiIi0JIE6VNFiAodt27bx97//nT/84Q+efQ6Hg8TERK9yNZ8dDke9dWVmZhIdHe3ZkpOTm6bRIiIiJxm/Bw7Tp08/bnS0efNmr3N27tzJZZddxrXXXsvEiRMb3YaMjAyKi4s9W0FBQaPrFBER8adAzTj4fY7D1KlTGTdu3DHLdO/e3fP3n3/+maFDhzJo0CBefPFFr3JJSUkUFhZ67av5nJSUVG/9NpsNm83WwJaLiIg0n0B9HNPvgUN8fDzx8fE+ld25cydDhw6lf//+zJs3r9Ykj5SUFO6//36qqqoICQkBICsrix49ehAbG+vvpouIiDSbQA0cTJvjsHPnToYMGULnzp2ZPXs2e/bsweFweM1duOGGGwgNDWXChAnk5eXx9ttv8/TTTzNlyhSzmi0iInJSM+1xzKysLLZt28a2bdvo1KmT1zHDMACIjo5myZIlpKen079/f+Li4njggQf0KKaIiAS8QM04mBY4jBs37rhzIQD69u3L559/3vQNEhERaUaBGji0mMcxRUREpOXTS65ERERMEKgZBwUOIiIiJgjUwEFDFSIiIuIzZRxERERMEKgZBwUOIiIiJgjUwEFDFSIiIuIzZRxERERMYLFYar1q4UTqaG4KHEREREwQqEMVChxERERMEKiBg+Y4iIiIiM+UcRARETFBoGYcFDiIiIiYIFADBw1ViIiIiM+UcRARETFBoGYcFDiIiIiYIFADBw1ViIiIiM+UcRARETFBoGYcFDiIiIiYIFADBw1ViIiIiM+UcRARETFBoGYcFDiIiIiYwGq1NvrtmI09/0QocBARETFBoGYcNMdBREREfKaMg4iIiAkCNeOgwEFERMQEgRo4aKhCREREfKaMg4iIiAkCNeOgwEFERMQEgRo4aKhCREREfKaMg4iIiEnMyBg0lgIHERERE2ioQkRERFq05557jr59+2K327Hb7aSkpPDpp582qI4WEThUVFTQr18/LBYLubm5Xse++eYbLrzwQsLCwkhOTubxxx83p5EiIiJ+VJNxaOzWEJ06deLRRx8lJyeHr776imHDhnHVVVeRl5fncx0tYqjivvvuo0OHDqxfv95rf0lJCSNGjGD48OE8//zzbNiwgfHjxxMTE8Ott95qUmtFREQaz59DFSUlJV77bTYbNputVvkrrrjC6/OsWbN47rnnWLNmDWeccYZP1zQ94/Dpp5+yZMkSZs+eXevYP//5TyorK3n11Vc544wzuP7667nrrrt48sknTWipiIiI/9S8HbOxG0BycjLR0dGeLTMz87jXdzqdvPXWW5SWlpKSkuJzu03NOBQWFjJx4kTef/99IiIiah3Pzs7moosuIjQ01LMvNTWVxx57jAMHDhAbG1tnvRUVFVRUVHg+Hx2JiYiItCYFBQXY7XbP57qyDTU2bNhASkoK5eXlREZG8t5779G7d2+fr2VaxsEwDMaNG8dtt93GgAED6izjcDhITEz02lfz2eFw1Ft3ZmamV+SVnJzsv4aLiIj4gT/nONRMdqzZjhU49OjRg9zcXL788ksmTZrE2LFj2bRpk8/t9nvgMH369ON2cvPmzfz973/n4MGDZGRk+LsJZGRkUFxc7NkKCgr8fg0REZHGMGNyJEBoaCinnnoq/fv3JzMzk7POOounn37a5/P9PlQxdepUxo0bd8wy3bt3Z/ny5WRnZ9eKigYMGMCYMWN47bXXSEpKorCw0Ot4zeekpKR6669vUoiIiIh4c7lcXsP7x+P3wCE+Pp74+Pjjlvvb3/7GX/7yF8/nn3/+mdTUVN5++20GDhwIQEpKCvfffz9VVVWEhIQAkJWVRY8ePeqd3yAiIhIIzFgAKiMjg5EjR9K5c2cOHjzIm2++ycqVK1m8eLHPdZg2ObJz585enyMjIwE45ZRT6NSpEwA33HADDz30EBMmTGDatGls3LiRp59+mqeeeqrZ2ysiIuJPRz4V0Zg6GmL37t3cfPPN7Nq1i+joaPr27cvixYu59NJLfa6jRazjUJ/o6GiWLFlCeno6/fv3Jy4ujgceeEBrOIiIiJyAV155pdF1tJjAoWvXrhiGUWt/3759+fzzz01okYiISNMJ1HdVtJjAQURE5GQSqIGD6StHioiISOBQxkFERMQEgZpxUOAgIiJiAgUOIiIi4jMzHsf0B81xEBEREZ8p4yAiImICDVWIiIhIg5hx428sDVWIiIiIz5RxEBERMYGGKkRERMRneqpCREREWj1lHEREREygoQoRERHxWaAGDhqqEBEREZ8p4yAiImKCQM04KHAQERExgQIHERER8ZkexxQREZFWTxkHERERE2ioQkRERHwWqIGDhipERETEZ8o4iIiImCBQMw4KHEREREygpypERESk1VPGQURExAQaqhARERGfBWrgoKEKERER8ZkyDiIiIiYI1IyDAgcRERETKHAQERERn1kslkY/Tqk5DiIiItKiKeMgIiJigkAdqjA94/Dxxx8zcOBAwsPDiY2NZdSoUV7H8/PzSUtLIyIigoSEBO69916qq6vNaayIiIif1AQOjd2am6kZh3fffZeJEyfyyCOPMGzYMKqrq9m4caPnuNPpJC0tjaSkJFavXs2uXbu4+eabCQkJ4ZFHHjGx5SIiIicn0wKH6upq7r77bp544gkmTJjg2d+7d2/P35csWcKmTZtYunQpiYmJ9OvXjz//+c9MmzaNBx98kNDQUDOaLiIi0mgaqmigr7/+mp07d2K1Wjn77LNp3749I0eO9Mo4ZGdnc+aZZ5KYmOjZl5qaSklJCXl5efXWXVFRQUlJidcmIiLSktS85KqxW7O3u9mveNj27dsBePDBB5kxYwYLFy4kNjaWIUOGsH//fgAcDodX0AB4PjscjnrrzszMJDo62rMlJyc3US9EREROLn4PHKZPn37ciRybN2/G5XIBcP/993PNNdfQv39/5s2bh8ViYcGCBY1qQ0ZGBsXFxZ6toKDAH10TERHxG02OPGzq1KmMGzfumGW6d+/Orl27AO85DTabje7du5Ofnw9AUlISa9eu9Tq3sLDQc6w+NpsNm812Is0XERGRY/B74BAfH098fPxxy/Xv3x+bzcaWLVu44IILAKiqquKHH36gS5cuAKSkpDBr1ix2795NQkICAFlZWdjtdq+AQ0RERJqHaU9V2O12brvtNmbOnElycjJdunThiSeeAODaa68FYMSIEfTu3ZubbrqJxx9/HIfDwYwZM0hPT1dGQUREAlqgPlVh6joOTzzxBMHBwdx0002UlZUxcOBAli9fTmxsLABBQUEsXLiQSZMmkZKSQps2bRg7diwPP/ywmc0WERFpNAUOJyAkJITZs2cze/bsest06dKFTz75pBlbJSIi0vQCNXAwfclpERERCRx6yZWIiIgJAjXjoMBBRETEBIEaOGioQkRERHymjIOIiIgJlHEQERERn5mx5HRmZibnnnsuUVFRJCQkMGrUKLZs2dKgOhQ4iIiInCQ+++wz0tPTWbNmDVlZWVRVVTFixAhKS0t9rkNDFSIiIieJRYsWeX2eP38+CQkJ5OTkcNFFF/lUhwIHERERE/hzjkNJSYnXfl9f9lhcXAxA27Ztfb6mhipEREQCXHJyMtHR0Z4tMzPzuOe4XC4mT57M4MGD6dOnj8/XUsZBRETEBP7MOBQUFGC32z37fck2pKens3HjRr744osGXVOBg4iIiAn8GTjY7XavwOF47rjjDhYuXMiqVavo1KlTg66pwEFERMQkzb0Og2EY3Hnnnbz33nusXLmSbt26NbgOBQ4iIiInifT0dN58800++OADoqKicDgcAERHRxMeHu5THZocKSIiYgIzFoB67rnnKC4uZsiQIbRv396zvf322z7XoYyDiIiICcxYctowjEZdD5RxEBERkQZQxkFERMQEesmViIiItHoKHERERMRnGqoQERExQaAOVShwEBERMUGgBg4aqhARERGfKeMgIiJigkDNOChwEBERMYECBxEREfFZoAYOmuMgIiIiPlPGQURExASBmnFQ4CAiImKCQA0cNFQhIiIiPlPGQURExATKOIiIiEirp8BBREREfGZq4PDdd99x1VVXERcXh91u54ILLmDFihVeZfLz80lLSyMiIoKEhATuvfdeqqurTWqxiIiIf9QMVTR2a26mBg6XX3451dXVLF++nJycHM466ywuv/xyHA4HAE6nk7S0NCorK1m9ejWvvfYa8+fP54EHHjCz2SIiIo2mwKGB9u7dy9atW5k+fTp9+/bltNNO49FHH+WXX35h48aNACxZsoRNmzbxxhtv0K9fP0aOHMmf//xn5s6dS2VlpVlNFxEROWmZFji0a9eOHj168Prrr1NaWkp1dTUvvPACCQkJ9O/fH4Ds7GzOPPNMEhMTPeelpqZSUlJCXl5evXVXVFRQUlLitYmIiLQkgZpxMO1xTIvFwtKlSxk1ahRRUVFYrVYSEhJYtGgRsbGxADgcDq+gAfB8rhnOqEtmZiYPPfRQ0zVeRESkkfQ45mHTp08/bnS0efNmDMMgPT2dhIQEPv/8c9auXcuoUaO44oor2LVrV6PakJGRQXFxsWcrKCjwU+9ERET8QxmHw6ZOncq4ceOOWaZ79+4sX76chQsXcuDAAex2OwDPPvssWVlZvPbaa0yfPp2kpCTWrl3rdW5hYSEASUlJ9dZvs9mw2WyN64iIiIjU4vfAIT4+nvj4+OOW++WXXwCwWr2THlarFZfLBUBKSgqzZs1i9+7dJCQkAJCVlYXdbqd3795+brmIiEjz0VBFA6WkpBAbG8vYsWNZv3493333Hffeey87duwgLS0NgBEjRtC7d29uuukm1q9fz+LFi5kxYwbp6enKKIiIiJjAtMAhLi6ORYsWcejQIYYNG8aAAQP44osv+OCDDzjrrLMACAoKYuHChQQFBZGSksKNN97IzTffzMMPP2xWs0VERE5qpr7kasCAASxevPiYZbp06cInn3zSTC0SERFpPmYMNTSW3o4pIiJiAs1xEBERkVZPgYOIiIj4TEMVIiIiJtBQhYiIiLR6yjiIiIiYIFAzDgocRERETBCogYOGKkRERMRnChxERETEZxqqEBERMUGgDlUocBARETFBoAYOGqoQERERnylwEBEREZ9pqEJERMQEGqoQERGRVk8ZBxERERMEasZBgYOIiIgJAjVw0FCFiIiI+EyBg4iIiPhMQxUiIiImCNShCgUOIiIiJgjUwEFDFSIiIuIzZRxERERMoIyDiIiItGirVq3iiiuuoEOHDlgsFt5///0G16HAQURE5CRRWlrKWWedxdy5c0+4Dg1ViIiImMCMoYqRI0cycuTIRl1TgYOIiIgJ/Bk4lJSUeO232WzYbLZG1V0fDVWIiIgEuOTkZKKjoz1bZmZmk11LGQcREZEAV1BQgN1u93xuqmwDKHAQERExhT+HKux2u1fg0JQ0VCEiIiI+U8ZBRETEBGY8VXHo0CG2bdvm+bxjxw5yc3Np27YtnTt39qkOBQ4iIiImMCNw+Oqrrxg6dKjn85QpUwAYO3Ys8+fP96mOJhuqmDVrFoMGDSIiIoKYmJg6y+Tn55OWlkZERAQJCQnce++9VFdXe5VZuXIl55xzDjabjVNPPdXnjomIiIi3IUOGYBhGra0h99YmCxwqKyu59tprmTRpUp3HnU4naWlpVFZWsnr1al577TXmz5/PAw884CmzY8cO0tLSGDp0KLm5uUyePJnf//73LF68uKmaLSIiIsdgMQzDaMoLzJ8/n8mTJ1NUVOS1/9NPP+Xyyy/n559/JjExEYDnn3+eadOmsWfPHkJDQ5k2bRoff/wxGzdu9Jx3/fXXU1RUxKJFi3xuQ0lJCdHR0RQXFzfbrFMREQlMTX3PqKl/x44dja6/pKSEbt26Nev9zbSnKrKzsznzzDM9QQNAamoqJSUl5OXlecoMHz7c67zU1FSys7OPWXdFRQUlJSVem4iISEtSM8ehsVtzMy1wcDgcXkED4PnscDiOWaakpISysrJ6687MzPRaQSs5OdnPrRcRETk5NShwmD59+nEjn82bNzdVW32WkZFBcXGxZysoKDC7SSIiIq1Cgx7HnDp1KuPGjTtmme7du/tUV1JSEmvXrvXaV1hY6DlW82fNviPL2O12wsPD6627KV/uISIi4g9mPI7pDw0KHOLj44mPj/fLhVNSUpg1axa7d+8mISEBgKysLOx2O7179/aU+eSTT7zOy8rKIiUlxS9tEBERkYZpsjkO+fn55Obmkp+fj9PpJDc3l9zcXA4dOgTAiBEj6N27NzfddBPr169n8eLFzJgxg/T0dE+24LbbbmP79u3cd999bN68mWeffZZ33nmHe+65p6maLSIi0iwCdXJkk60c+cADD/Daa695Pp999tkArFixgiFDhhAUFMTChQuZNGkSKSkptGnThrFjx/Lwww97zunWrRsff/wx99xzD08//TSdOnXi5ZdfJjU1tamaLSIiIsfQ5Os4tARax0FERHzVXOs45Ofn+2Udh86dOzfr/U3vqhARETFBoE6O1Gu1RURExGfKOIiIiJhAGQcRERFp9RQ4iIiIiM80VCEiImICDVWIiIhIq6fAQURERHymoQoRERETaKhCREREWj0FDiIiIuIzDVWIiIiYQEMVIiIi0uopcBARERGfaahCRETEBBqqEBERkVZPgYOIiIj4TEMVIiIiJtBQhYiIiLR6ChxERETEZwocRERExGea4yBykisrg507YccOqK4Gux3i46F7dwg+4hvC6YSXXoLt2+H00+G00yApCXr0cB/Py4Mff4S4OBgwAJYsgaVLYfdu91ZcDGecAY8+Crt2QU4OhIbC8OFQWAh//CN8/TVYre6627Z117lrF1RVQXg4XHih++9btkBpKbhcEBbmPqe62t2OLl3g7LPh0CHYuhUOHnS3KSEBVq6EkhKwWCA2Fk49FYqK3G0LD4e0NJg+HRITm/tfQU5GgTrHwWIYhtHsV21mJSUlREdHU1xcjN1uN7s5chw//QSrVoFhQEqK+wYm/rd/P8ycCS+/DOXlLsByeHMBEBfn4t57g5k6FcaMgX//24nTGXT4bAOLBQzDQs+e7vKbN/+awAwOdlFdbQWch+sLOeLKVbh/Z/n1C89qdeFyWbFa3fU6nZbD5wYBFUAlEHX4XBdQDCQATiwWA8MIJjh4PxZLJVVVSYfLVQERhIaWU1kZDAQTFrYNi8WgrOy0w8dDgO3A90B/oC1Wq8Frr1m48UZ//JQlEDX1PaOm/t27dze6/pKSEhISEpr1/qaMg7QYxcUwaZLBO+/U3DjAYjG44gp4+WUL8fEmN7AV2b/f/dv7jz9CdbWTsLCfSEj4J2FhOygrO53du69n374Epk2z8tRT4HBY6dlzJ8OGfUNs7EF27EhiyZJzKCkJ57vvgunYcS+33voVXbrsYc+eaLKy+pGX1xX3jf8N4EncN+jzgAeBQVx0kZWtWy3s3m0QGQl/+UsJV15ZjtUKw4dHs2VLFXAn8BZQDgwC5gCnAilAH+BdgoP307XrTKKi1mGxQFnZKfz44wxKS3sAj1NZOYXw8G10734/YWE/AvDdd3M4eLAf8Bvg48M/lVDgblyux7n5ZoPu3S0MGtT0/xYigUYZB2kRKith6FCDjRsNMjJKufrqcoKC4KOPbGRmRpKQYCE720JkpNktbR3uuQfmzYO4uGoKCzdz2ml/wGqt8Bx3OtuwZctLuFyhVFZ25txzv+Pmm5dzZFb00CEbM2bcTGJiEX/8438ICXF6jhkGvPrqpeTmdsLlisGdMagRAqwG+uGeZmVl8eL99OvnHmv47LMQrrsuFhgJLDqq5XbgW+AD4EzgAnr3/h3h4d97lXI627Bhw0c4nQVYLKdy5pmXExJyAIDy8mTy8t4DJgCv1vHTmU9Q0GiuuCKE995r/jSwmK+5Mg579uzxS8YhPj6+We9vmhwpLcK778Lq1RbefLOI8ePLiIkxiIoyuOGGcv797wN8+y3Mn292K1uHigr3z/LKK+H774Np336uV9AAEBRUSvv2L1NR0RXDsHD55es4eii1tDScqqoQUlNzvIIGcM8hSEv7CpcrHEg7qgVVwKNAMFarhUGDKjxBA8B774URHLyV2kEDQAnum/0NwEAiI/9XK2ioaX+7dh8BpxETs8ITNAAUFw/GncF4s56f0Ks4naEsXOgOgETEmwIHaRFef91g0KBKzj23utaxHj2cXHZZBa+9pm9xf9i92z0hsGboJyrq6zrLRUXlABAWVkG7dgdrHT9woA0AnTvvrfP8pKQDBAVVAsl1HN0IuCc1nnKKy+tIUZEVp7N2MPCrHUA0YCUkZHe9pUJDd2OxQHBwkdd+wwjFHTiU13Omu3x1tQWXq54iIicxBQ7SIjgccPrptYOGGj16OHE4mrFBrVjNcE/54ftmZWVCneUqKxMP/xlCWVloreNRUe4KCgtj6jx///5InM5QoK6bex/A/aTG118He/1m37mzk6CgAXhPqDzSQOBHoJRDh87BMILqLFVSci6GUUVJyfle9UdEbAFicM+TqMv/YbE46d3bIKjuqkX8ouapisZuzU2Bg7QIHTrApk313SggLy+Yjh2bsUGtWGys+xHI7Gxo08bJnj3X1Vluz55rCQoqwuWy8vnnZ9Q6npS0H5utkqysfrhctb+8srLOxmKpAD466kgIMA2oxjAgLy+ExYt/DUzGjCmjujoO98TIo/UCxgCvAK9SVRVPYeHoWqVKSs6lpGQw8C8qKrqyZ8/vPMeiotYSGvoj8DTuJzWO1BOr9T4Mw8rkyZrfIFIXBQ7SIowbZ2Ht2hC++KJ28LBxYzBZWaGMG6cvcn+5/35Yvx66drVSWHgTu3ZNwOl0Dz1UV9vZufN29u79DVarQVCQiw8/PJ9PP+1PaakNgF27YnnllVQqKoLZurUjL7wwkvz8OAwD9u6189ZbF7Fq1ZkYhg2Yi3siYxtgKLAUOIfBg4M44wwLVquLCROi+ctf2rBpUxBWK5x3XiXwV2AecCHQF8gA/os7g7EKcADV7Nw5me+/f5yioospKRlIfv40tm79++HjFwNOCgruZevWv7NvXxr7948kJGQ/cDawFfdTHmOB54CvcbkiueIKGD++6f8dRAKRnqqQFqG6GkaMMFi7FqZMKWXUqHKCg91PVfz1r23o3t3CqlUWIiLMbmnrsWCB++Z46JCBe12GakJC9lFVFXc4/W+lV69qFiwIZsQIg127AAxCQ6upqAjFanXicgURFuYiLMw9N8FqNXC5LJ51Gepfx+HXz1YruFzuYYEjH8N1fzU5jyhbMwEz6IjPVsCCxVKFYdSUq8b9pLkLMDzlLZZqDCP4qL8bh8sFAVUkJATzpz9ZuOMONExxEmuupyr27dvnl6cq2rVr16z3NwUO0mKUlsLkyQb/+AdUVLhvIMHBBtddB3PnWoiJMbd9rdGhQ/Cvf8FHH8GGDe6nCGJiYMgQ91MXQ4fieZpizRqYPBn27XPPk7jqKvcKj1dd5V69MSvr15UjR46EDz6A5cthzx736o2HDkHfvvDYY+5/69xcsNlg8GD3XIeXX4aPP4Y2bdzX79jRvfLjN9/A3r3uFSpvv93dxmXL3NeprHSvHjl8OKxb567v4ovdq0PGxroD0p9/dtcVG+uuZ88e9+qYsbEQEuJeHbOy0v2nggUBBQ7Ho8BBWpy9e2H1avcN4rzzoH17s1skIicTBQ7H1mRzHGbNmsWgQYOIiIggpo5fFdevX8/o0aNJTk4mPDycXr168fTTT9cqt3LlSs455xxsNhunnnoq8/Uwf6sXF+f+bfeqqxQ0iEjrpacqjlJZWcm1117LpEmT6jyek5NDQkICb7zxBnl5edx///1kZGTwzDPPeMrs2LGDtLQ0hg4dSm5uLpMnT+b3v/89ixcvbqpmi4iIyDE0+VDF/PnzmTx5MkVFRcctm56ezrfffsvy5csBmDZtGh9//DEbN270lLn++uspKipi0aK6VpWrm4YqRETEV801VLF//36/DFW0bdu2dQxVnIji4mLatm3r+Zydnc3w4cO9yqSmppKdnX3MeioqKigpKfHaREREWhINVTTS6tWrefvtt7n11ls9+xwOB4mJiV7lEhMTKSkpoaysrN66MjMziY6O9mzJyXUteSsiIiIN1aDAYfr06ceNfDZv3tzgRmzcuJGrrrqKmTNnMmLEiAaff7SMjAyKi4s9W0FBQaPrFBEREfcqKT6bOnUq48aNO2aZ7t27N6gBmzZt4pJLLuHWW29lxowZXseSkpIoLCz02ldYWIjdbic8PLzeOm02GzabrUHtEBERaU7+GGowY6iiQYFDfHw88TWv1PODvLw8hg0bxtixY5k1a1at4ykpKXzyySde+7KyskhJqe/lNCIiItKUmmyOQ35+Prm5ueTn5+N0OsnNzSU3N5dDhw4B7uGJoUOHMmLECKZMmYLD4cDhcLBnzx5PHbfddhvbt2/nvvvuY/PmzTz77LO888473HPPPU3VbBERkWZh1uTIuXPn0rVrV8LCwhg4cCBr165tWAVGExk7dqx7AfyjthUrVhiGYRgzZ86s83iXLl286lmxYoXRr18/IzQ01Ojevbsxb968BreluLjYAIzi4uLGd0xERFq1pr5n+LP+htb11ltvGaGhocarr75q5OXlGRMnTjRiYmKMwsJCn695Uiw5XVxcTExMDAUFBVrHQUREjqmkpITk5GSKioqIjo5ukvqjo6P9ck+qaevRddU312/gwIGce+65nsUWXS4XycnJ3HnnnUyfPt2nazZojkOgOnjwIIAeyxQREZ8dPHiwSQKH0NBQkpKS/HZPioyMrFXXzJkzefDBB732VVZWkpOTQ0ZGhmef1Wpl+PDhx10f6UgnReDQoUMHCgoKiIqKatQM1Poiu0DV2voDra9Pra0/0Pr61Nr6A62vTw3tj2EYHDx4kA4dOjRJe8LCwtixYweVlZV+qc8wjFr3trqyDXv37sXpdNa5PlJDllI4KQIHq9VKp06d/Faf3W5vFf8z1Wht/YHW16fW1h9ofX1qbf2B1tenhvSnKTINRwoLCyMsLKxJr9FUWszKkSIiItJ04uLiCAoKqnN9pKSkJJ/rUeAgIiJyEggNDaV///4sW7bMs8/lcrFs2bIGrY90UgxV+IvNZmPmzJmtZlXK1tYfaH19am39gdbXp9bWH2h9fWpt/WmMKVOmMHbsWAYMGMB5553HnDlzKC0t5ZZbbvG5jpPicUwRERFxe+aZZ3jiiSdwOBz069ePv/3tbwwcONDn8xU4iIiIiM80x0FERER8psBBREREfKbAQURERHymwEFERER8dlIHDrNmzWLQoEFEREQQExNTZ5m77rqL/v37Y7PZ6NevX51lDMNg9uzZnH766dhsNjp27MisWbOOee39+/czZswY7HY7MTExTJgwwfPKcTP788MPP9T52tY1a9Yc89rr1q3jkksuISYmhtjYWFJTU1m/fn2j+mN2nwDmz59P3759CQsLIyEhgfT09IDuD8C+ffvo1KkTFouFoqKiE+/MYWb1af369YwePZrk5GTCw8Pp1asXTz/9dMD2ByA/P5+0tDQiIiJISEjg3nvvpbq62vT+HGnbtm1ERUXVW9eRWvL3wpEa0ifw//dCoDmpA4fKykquvfZaJk2adMxy48eP53e/+129x++++25efvllZs+ezebNm/nwww8577zzjlnnmDFjyMvLIysri4ULF7Jq1SpuvfXWE+pHDX/1B2Dp0qXs2rXLs/Xv37/esocOHeKyyy6jc+fOfPnll3zxxRdERUWRmppKVVXVCfWlhll9AnjyySe5//77mT59Onl5eSxdupTU1NQG9+FIZvanxoQJE+jbt6/PbT4es/qUk5NDQkICb7zxBnl5edx///1kZGR43vp3oszqj9PpJC0tjcrKSlavXs1rr73G/PnzeeCBB06oHzX82Z+qqipGjx7NhRdeeNzrBsr3QkP6BE3zvRBwGvjq71Zp3rx5RnR09DHLzJw50zjrrLNq7d+0aZMRHBxsbN682efrbdq0yQCMdevWefZ9+umnhsViMXbu3OlzPfVpTH927NhhAMb//vc/n6+3bt06AzDy8/M9+7755hsDMLZu3epzPcfS3H3av3+/ER4ebixdurRhDfVRc/enxrPPPmtcfPHFxrJlywzAOHDgQIPrqI9ZfTrS7bffbgwdOrRRddRo7v588sknhtVqNRwOh2ffc889Z9jtdqOiosLneurTmP7UuO+++4wbb7zRp7pa+vdCjYb0qam/FwLFSZ1x8IePPvqI7t27s3DhQrp160bXrl35/e9/z/79++s9Jzs7m5iYGAYMGODZN3z4cKxWK19++WVzNPu4rrzyShISErjgggv48MMPj1m2R48etGvXjldeeYXKykrKysp45ZVX6NWrF127dm2eBvugIX3KysrC5XKxc+dOevXqRadOnbjuuusoKChoptYeX0P6A7Bp0yYefvhhXn/9dazWlvm/fkP7dLTi4mLatm3bBC07MQ3pT3Z2NmeeeabXmwtTU1MpKSkhLy+vqZt6XMuXL2fBggXMnTvXp/KB8L3Q0D4FwvdCc2iZ3x4BZPv27fz4448sWLCA119/nfnz55OTk8Nvf/vbes9xOBwkJCR47QsODqZt27Y4HI6mbvIxRUZG8te//pUFCxbw8ccfc8EFFzBq1KhjfulFRUWxcuVK3njjDcLDw4mMjGTRokV8+umnBAebv6r5ifRp+/btuFwuHnnkEebMmcO///1v9u/fz6WXXuq3V+GeqBPpT0VFBaNHj+aJJ56gc+fOzdha35xIn462evVq3n777UYP+fnDifTH4XDU+brjmmNm2rdvH+PGjWP+/Pk+v12ypX8vnEifWvL3QnNqdYHD9OnT65yUdOTWkPeOH4/L5aKiooLXX3+dCy+8kCFDhvDKK6+wYsUKtmzZ0uj6m7s/cXFxTJkyhYEDB3Luuefy6KOPcuONN/LEE0/Ue05ZWRkTJkxg8ODBrFmzhv/+97/06dOHtLQ0ysrKArJPLpeLqqoq/va3v5Gamsr555/Pv/71L7Zu3cqKFSsCrj8ZGRn06tWLG2+80adrBEKfjrRx40auuuoqZs6cyYgRIwK+P8fT3P2ZOHEiN9xwAxdddJHP57T074UT6VNDvhdaM/PDPj+bOnUq48aNO2aZ7t27++167du3Jzg4mNNPP92zr1evXoB7hnSPHj1qnZOUlMTu3bu99lVXV7N///5arzZt7v7UZeDAgWRlZdV7/M033+SHH34gOzvbkwJ/8803iY2N5YMPPuD666/3Kh8IfWrfvj0AvXv39uyLj48nLi6O/Px8r7KB0J/ly5ezYcMG/v3vfwPuJ4HAfYO7//77eeihh7zKB0KfamzatIlLLrmEW2+9lRkzZtRZJhD6k5SUxNq1a7321bz+2OzvheXLl/Phhx8ye/ZswP3fj8vlIjg4mBdffJHx48fXOqelfy+cSJ8a8r3QmrW6wCE+Pp74+Phmu97gwYOprq7m+++/55RTTgHgu+++A6BLly51npOSkkJRURE5OTmeWdbLly/H5XLVetFIc/enLrm5uZ7/Yeryyy+/YLVasVgsnn01n10uV63ygdCnwYMHA7BlyxY6deoEuB+h3bt3b61/10Doz7vvvuv1W966desYP348n3/+uee/2yMFQp8A8vLyGDZsGGPHjj3mI9CB0J+UlBRmzZrF7t27PUOZWVlZ2O12rxsVNH9/srOzcTqdns8ffPABjz32GKtXr6Zjx451ntPSvxdOpE8N+V5ozVpd4NAQ+fn57N+/n/z8fJxOJ7m5uQCceuqpREZGAu7new8dOoTD4aCsrMxTpnfv3oSGhjJ8+HDOOeccxo8fz5w5c3C5XKSnp3PppZd6shBr167l5ptvZtmyZXTs2JFevXpx2WWXMXHiRJ5//nmqqqq44447uP766+nQoYOp/XnttdcIDQ3l7LPPBuA///kPr776Ki+//LLnOu+99x4ZGRmetOGll17KvffeS3p6OnfeeScul4tHH32U4OBghg4desL9MbNPp59+OldddRV33303L774Ina7nYyMDHr27NmoPpnVn6ODg7179wLu7Jivz663tD5t3LiRYcOGkZqaypQpUzzzAIKCghp1AzKrPyNGjKB3797cdNNNPP744zgcDmbMmEF6enqjXgftj/7UZFFrfPXVV1itVvr06VNvf1r698KJ9KmpvhcCjslPdZhq7NixBlBrW7FihafMxRdfXGeZHTt2eMrs3LnTuPrqq43IyEgjMTHRGDdunLFv3z7P8RUrVtQ6Z9++fcbo0aONyMhIw263G7fccotx8OBB0/szf/58o1evXkZERIRht9uN8847z1iwYIHXdebNm2cc/Z/OkiVLjMGDBxvR0dFGbGysMWzYMCM7O7tR/TG7T8XFxcb48eONmJgYo23btsZvfvMbr0fLAq0/R6r5b9Ifj2Oa1aeZM2fWWWeXLl0Csj+GYRg//PCDMXLkSCM8PNyIi4szpk6dalRVVZnen6PV9ehioH0vnGifmuJ7IdDotdoiIiLis1b3VIWIiIg0HQUOIiIi4jMFDiIiIuIzBQ4iIiLiMwUOIiIi4jMFDiIiIuIzBQ4iIiLiMwUOIiIi4jMFDiIiIuIzBQ4iIiLiMwUOIiIi4rP/D00QZDm9OD0sAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "coordinate_x = torch.tensor(training_input[:,0].tolist() + testing_input[:,0].tolist())\n",
    "coordinate_y = torch.tensor(training_input[:,1].tolist() + testing_input[:,0].tolist())\n",
    "day = torch.tensor(training_input[:,4].tolist() + testing_input[:,4].tolist())\n",
    "plt.scatter(coordinate_x,coordinate_y, c=day, cmap=\"Greys\", edgecolor=\"blue\")\n",
    "plt.colorbar()\n",
    "#unique_days is initalized when making the original df (took from pd.factorize on days of week)\n",
    "print(unique_days)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d2985ae1-332a-4334-9939-5b4885945149",
   "metadata": {},
   "outputs": [],
   "source": [
    "#^^^\n",
    "#There appears to be a pattern of location on the scatter space. Many of the values are scattered close together, with two distinct area located at the\n",
    "#top and bottom. This implies that these certain areas tend to have plenty of problems/incidents. In these spaces, there appears to be days of all \n",
    "#type that are involved in these location. However, based on the cmap, days represented to be darker seem to appear most often, especially on the bottom\n",
    "#space. Since the factorize days are not ordered in a meaningful manner (Monday, Tuesday, Wednesday, etc.), I need to look at the actual days\n",
    "#corresponding to the indexes rather than rely on the color tone and the scale of it. This means that Wednesday, Monday, Sunday, and Saturday appear\n",
    "#to be days where incident tend to happen being that those day are dark. This is however mainly a general guess. What I can observe about any patterns\n",
    "#of the days are from the few outliers appearing outside the two main spaces. These outliers tend to be of lighter color, which contrasts the other half\n",
    "#of the days (the darker colors) by being common outside the main spaces. In other words, it is ironic that the darker colors (using colors just to \n",
    "#efer to the one half of the days set) are not too common in the outliers despite appearing dominant in the main spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "99853008-0c8d-4e33-b592-d24740997edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x19878677ee0>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#number of people involve after factor (factorize heavily simplies how many people involve by distinguishing only unique values and not the actual\n",
    "#how many people involve)\n",
    "people = torch.tensor(training_label[:,0].tolist() + testing_label[:,0].tolist())\n",
    "plt.scatter(coordinate_x,coordinate_y, c=people, cmap=\"Greys\", edgecolor=\"blue\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "04cd3826-9c63-4361-8471-93eeae5a1bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#^^^\n",
    "#Looking at the actual colors, being that this time the color scale matters (unlike the days if not including the use to distinguish half of the set)\n",
    "#and understanding that there is 2 main common spaces of incidents (top and bottom), it appears that many incident are very common to the lighter side\n",
    "#of the classification for how many people is involve. Despite the var people being factorize, many of the lighter/lower colors do have a correspondance\n",
    "#with how frequent certain numbers of people are involved. This meaning that the lighter colors are also low in value for how many people are involved.\n",
    "#0 is both the lightest color determined by the factorization, the most common incident number in the actual set, and the lowest casualty realistically\n",
    "#As stated before when talking about VictimCount, 0 or 1 or 2 people could be implied to be of accidents or small incident and higher values of like 10\n",
    "#implies of murder sprees. With these distinction, accident or small incident can intuitive be understood to occur less than murder sprees.\n",
    "#Understanding this about the lighter colors, it makes sense in the plot that one common pattern is that many of the incidents tends to be of the \n",
    "#lighter side. In contrast of this, the darkest color, 6, is/are barely visible from a general look, further emphasizing the rarity of a high people\n",
    "#count incident. What could also be pointed are in the outliers. It may be a little suprising that the outliers on the top have no 0/white colors, \n",
    "#despite white being very dominant. These areas on the top outliers may tell a story that despite not living in an area full of incidents (which may be\n",
    "#judged as dangerous) is still not as peaceful as initially presume, being that people still got hurt. There is one outlier at the bottom which is 0,\n",
    "#which implies the initial judgement of the top outlier but true (or not yet proven false)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "08e5211a-8a61-43c9-adac-7c99313de6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Sunday', 51), ('Saturday', 44), ('Monday', 43), ('Friday', 40), ('Thursday', 37), ('Tuesday', 32), ('Wednesday', 28)]\n"
     ]
    }
   ],
   "source": [
    "#get the statistic of the days with the highest incident \n",
    "#counts all the incidents occuring on each day\n",
    "count, unique = np.unique(day, return_counts=True)\n",
    "temp_count = dict(zip(count,unique))\n",
    "temp_count = sorted(temp_count.items())\n",
    "#replace factorize index of days to the actual days to visualize easier\n",
    "day_count = {}\n",
    "for i in range(len(temp_count)):\n",
    "    day_count[unique_days[i]] = temp_count[i][1]\n",
    "day_count = sorted(day_count.items(),key = lambda day_count:day_count[1], reverse=True)\n",
    "#sorted by days with the highest incidents count first\n",
    "print(day_count)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b8e1e2-3820-4cb6-9fd4-7849abf2ea66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#It appears that Sunday is the day with the most frequent incident count. There is some jump from Sunday to Saturday (Saturday is the second highest) \n",
    "#with a jump of 7. This is the highest of any jump between two days, implying that Sunday is a special/more common day for incidents to happen.\n",
    "#I can also note that it is possible that the classification model may have found this pattern of Sunday being frequent during training.\n",
    "#I can also reflect that for the analysis of the first bar graph (about the days), I mention that the darker day, which I will now group as \n",
    "#the last 4 indexes of the color bar or the unique days as seen below, had appear to be most common on the bar graph. Looking at the actual statistics,\n",
    "#I can now see that this pattern is correct except for Wednesday, which actually has the lowest frequency. I can maybe use this insight on the \n",
    "#convolutional neural network, which can pick up patterns both good and bad. I feel that this is a potential example of a nn picking up a potentially \n",
    "#bad pattern for classification by judging darker values (which in the context of nn, I will replace darker values as the higher and lower index as \n",
    "#numbered index scale can be seen in nn and still corresponds with darker and lighter colors of the cmap) as being days with more incidents. Even if it \n",
    "#is true for the other darker values, it is not true for Wednesday. This thought may be of a stretch since I do not bring up/fully understand the \n",
    "#actual connection between the mathematical equations the nn use to pick up patterns and this thought of judging darker values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c7e116b6-b0aa-49e4-ac06-d99a184cb514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Friday', 'Thursday', 'Tuesday', 'Saturday', 'Sunday', 'Monday',\n",
      "       'Wednesday'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(unique_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aebdcdf-158e-454b-897d-70430fd5b471",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
